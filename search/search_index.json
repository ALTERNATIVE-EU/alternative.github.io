{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#horizon-2020-alternative-project","title":"Horizon 2020 ALTERNATIVE Project","text":"<p>Horizon 2020 research project ALTERNATIVE develops an innovative platform and an integrated approach for testing and assessment of the potential of chemicals to induce cardiotoxicity. The novel approach will strengthen the capacity of regulators and industry to prevent cardiotoxic co-exposures to industrial chemicals and pharmaceuticals in an effective way.</p>"},{"location":"#purpose-of-the-cloud-data-platform","title":"Purpose of the Cloud Data Platform","text":"<p>The Cloud Data Platform is the cornerstone of the ALTERNATIVE project, designed to facilitate the project's core objectives. It serves multiple purposes:</p> <ol> <li> <p>Central Hub for Data and ML Model Exchange: The platform acts as a central repository, enabling efficient and seamless exchange of data and ML models among consortium partners. This ensures that all participants have access to the latest resources, fostering collaboration and accelerating research progress.</p> </li> <li> <p>Innovative Environment for ML Model Development and Hosting: By providing a robust infrastructure for ML model hosting, the platform supports the development and deployment of advanced in-silico models for toxicity prediction. This environment is crucial for the iterative improvement of models and the validation of their predictions.</p> </li> <li> <p>Gateway for External Access to Project Resources: Beyond the lifespan of the ALTERNATIVE project, the platform aims to serve as a gateway for the broader research community. By exposing data and ML services through a well-defined API, it enables external users to leverage the project's outputs, thereby extending its impact and utility.</p> </li> </ol>"},{"location":"#key-objectives-of-the-cloud-data-platform","title":"Key Objectives of the Cloud Data Platform","text":"<ul> <li> <p>Facilitation of Data Exchange: A primary goal of the platform is to enable seamless and efficient data exchange between consortium partners throughout the duration of the ALTERNATIVE project. This feature is vital for ensuring that all participants have timely and easy access to the data necessary for the project's success.</p> </li> <li> <p>Hosting of ML In-Silico Models for Toxicity Prediction: The platform serves as a robust environment for hosting advanced Machine Learning (ML) in-silico models. These models are crucial for toxicity prediction, representing a significant stride in the field of predictive analysis and safety assessment. By providing a reliable and scalable hosting solution, the platform ensures that these models are readily accessible and operable for the project's researchers and analysts.</p> </li> <li> <p>Exposing Data and ML Services via API: Post-completion of the ALTERNATIVE project, the platform is designed to extend its utility beyond the consortium. It will expose data and ML services to external users through a well-defined Application Programming Interface (API). This extension will facilitate wider access to the project's valuable resources, thereby enhancing research and development efforts in related fields.</p> </li> </ul>"},{"location":"#structure-of-the-cloud-data-platform","title":"Structure of the Cloud Data Platform","text":"<p>To achieve these ambitious goals, the Cloud Data Platform is constructed as a multi-layered software stack, each layer playing a critical role in the platform's overall functionality and performance:</p> <ul> <li> <p>Cloud and Infrastructure Layer: This foundational layer provides the necessary cloud-based infrastructure to support the platform's operations, ensuring scalability, reliability, and security.</p> </li> <li> <p>Administrative Layer: The administrative layer is responsible for overseeing the platform's overall management, including user access control, resource allocation, and monitoring of platform activities.</p> </li> <li> <p>Data Layer: At the core of the platform is the data layer, which handles the storage, processing, and management of data. This layer is optimized for high-performance data operations, essential for the effective functioning of ML models and data exchange processes.</p> </li> <li> <p>APIs Layer: The APIs layer is designed to offer a streamlined and secure interface for accessing the platform's services. It plays a crucial role in integrating the platform with external systems and in making data and ML services available to external users post-project.</p> </li> <li> <p>Microservices Layer: This layer comprises a suite of microservices, each designed to perform specific functions within the platform. The use of microservices architecture enhances the platform's modularity, flexibility, and ease of maintenance.</p> </li> </ul> <p>In summary, the Cloud Data Platform developed for the ALTERNATIVE project represents a sophisticated and multi-faceted solution, tailored to meet the project's unique requirements for data exchange, ML model hosting, and service accessibility. This document will delve into each aspect of the platform, elucidating its design, functionalities, and the value it brings to the ALTERNATIVE project and its stakeholders.</p>"},{"location":"maintainer-guide/","title":"Getting started","text":"<p>The sections below document how to setup and maintain an instance based on ALTERNATIVE platform, including installing, upgrading and configuring its features and extensions.</p>"},{"location":"maintainer-guide/repositories/","title":"Repositories","text":"Repository Description Link AI/ML API Core REST API component with Flask application logic GitHub Envoy Custom Filter Go implementation of token-based authentication and authorization GitHub Keycloak AI/ML Access Tokens Plugin Keycloak extension for managing AI/ML API access tokens GitHub CKAN Platform Modified CKAN platform with token management features GitHub Keycloak Authorization Plugin Keycloak plugin for managing user roles and permissions GitHub CKAN Extension - ckanext-cloudstorage CKAN extension that implements support for S3 Cloud Storage GitHub CKAN Extension - ckanext-keycloak_auth CKAN extension that enables Keycloak authentication and user management GitHub CKAN Extension - ckanext-alternative_theme CKAN extension that changes the default theme of the platform to match the look of ALTERNATIVE GitHub CKAN Extension - ckanext-extrafields CKAN extension that adds additional fields to dataset metadata, such as size and experiment info GitHub Alternative Documentation Documentation about the ALTERNATIVE platform GitHub Alternative Python Library Python library to work with the Alternative platform GitHub"},{"location":"maintainer-guide/admin-services/cert-manager/","title":"Cert-manager","text":""},{"location":"maintainer-guide/admin-services/cert-manager/#overview","title":"Overview","text":"<p>Cert-manager is a crucial component of the ALTERNATIVE platform's security infrastructure, responsible for automating the management of SSL/TLS certificates. This powerful tool ensures that all communications within and to the platform remain encrypted and secure, playing a vital role in maintaining data integrity and confidentiality.</p>"},{"location":"maintainer-guide/admin-services/cert-manager/#key-features","title":"Key Features","text":"<ol> <li> <p>Automated Certificate Management: Cert-manager handles the entire lifecycle of SSL/TLS certificates, from issuance to renewal, without manual intervention.</p> </li> <li> <p>Integration with Let's Encrypt: Leverages the free, automated, and open Certificate Authority to provide trusted certificates.</p> </li> <li> <p>Kubernetes Native: Designed to work seamlessly within the Kubernetes ecosystem, aligning with the platform's cloud-native architecture.</p> </li> <li> <p>Support for Multiple Issuers: While primarily configured with Let's Encrypt, cert-manager can work with various certificate issuers.</p> </li> <li> <p>Certificate Rotation: Automatically handles certificate rotation before expiration, ensuring continuous secure communication.</p> </li> </ol>"},{"location":"maintainer-guide/admin-services/cert-manager/#how-cert-manager-works","title":"How Cert-manager Works","text":"<ol> <li> <p>Certificate Request: When a new service is deployed or a certificate is near expiration, cert-manager detects the need for a new certificate.</p> </li> <li> <p>ACME Challenge: Utilizes the ACME protocol to prove control over the domain to Let's Encrypt.</p> </li> <li> <p>Certificate Issuance: Once validated, Let's Encrypt issues the certificate, which cert-manager then stores as a Kubernetes secret.</p> </li> <li> <p>Distribution: The certificate is automatically distributed to the relevant services and ingress controllers.</p> </li> <li> <p>Monitoring and Renewal: Cert-manager continuously monitors certificate expiration dates and initiates the renewal process when necessary.</p> </li> </ol>"},{"location":"maintainer-guide/admin-services/cert-manager/#benefits-for-the-alternative-platform","title":"Benefits for the ALTERNATIVE Platform","text":"<ul> <li>Enhanced Security: Ensures all platform communications are encrypted, protecting sensitive research data.</li> <li>Reduced Administrative Overhead: Eliminates the need for manual certificate management, freeing up resources for core platform development.</li> <li>Scalability: Easily manages certificates for multiple services and domains as the platform grows.</li> <li>Compliance: Helps maintain compliance with data protection regulations by ensuring data in transit is always encrypted.</li> </ul>"},{"location":"maintainer-guide/admin-services/cert-manager/#configuration-and-management","title":"Configuration and Management","text":"<p>Cert-manager is configured declaratively through Kubernetes custom resources:</p> <ul> <li>Issuer/ClusterIssuer: Defines how cert-manager should request certificates.</li> <li>Certificate: Describes the desired X.509 certificate.</li> <li>CertificateRequest: A one-shot request for a certificate, typically managed automatically.</li> </ul> <p>Administrators can monitor cert-manager's operations through Kubernetes logs and events, ensuring smooth operation of this critical security component.</p>"},{"location":"maintainer-guide/admin-services/external-dns/","title":"External-DNS","text":""},{"location":"maintainer-guide/admin-services/external-dns/#overview","title":"Overview","text":"<p>External-DNS is a critical component of the ALTERNATIVE platform, automating DNS record management for applications and services. This automation is essential in our dynamic cloud environment, where service endpoints and IP addresses frequently change.</p>"},{"location":"maintainer-guide/admin-services/external-dns/#key-features-and-benefits","title":"Key Features and Benefits","text":"<ol> <li> <p>Automated DNS Management: </p> <ul> <li>Responds to changes in the platform environment in real-time</li> <li>Ensures DNS entries are always accurate and up-to-date</li> </ul> </li> <li> <p>Dynamic IP Address Mapping: </p> <ul> <li>Maps human-readable URLs to dynamically allocated IP addresses</li> <li>Enhances user experience through consistent and intuitive service access</li> </ul> </li> <li> <p>Integration with Security Infrastructure: </p> <ul> <li>Facilitates the issuance of TLS certificates by Cert-manager</li> <li>Contributes to the platform's robust security posture</li> </ul> </li> <li> <p>Cloud-Native Compatibility: </p> <ul> <li>Designed to work seamlessly in containerized and microservices architectures</li> <li>Supports various DNS providers and Kubernetes ingress controllers</li> </ul> </li> </ol>"},{"location":"maintainer-guide/admin-services/external-dns/#how-external-dns-works","title":"How External-DNS Works","text":"<ol> <li>Monitoring: Continuously watches for changes in services and ingresses within the Kubernetes cluster</li> <li>Record Creation/Update: Automatically creates or updates DNS records based on the observed changes</li> <li>Synchronization: Ensures DNS records in external DNS providers match the desired state in the cluster</li> </ol>"},{"location":"maintainer-guide/admin-services/external-dns/#integration-with-alternative-platform","title":"Integration with ALTERNATIVE Platform","text":""},{"location":"maintainer-guide/admin-services/external-dns/#dns-server-integration","title":"DNS Server Integration","text":"<ul> <li>External-DNS interfaces directly with the platform's DNS server</li> <li>Manages A, CNAME, and TXT records for services and ingresses</li> </ul>"},{"location":"maintainer-guide/admin-services/external-dns/#cert-manager-synergy","title":"Cert-manager Synergy","text":"<ul> <li>Provides necessary DNS entries for TLS certificate issuance</li> <li>Enables automated certificate management and renewal processes</li> </ul>"},{"location":"maintainer-guide/admin-services/external-dns/#security-enhancement","title":"Security Enhancement","text":"<ul> <li>Contributes to a secure communication environment</li> <li>Supports DNSSEC for added DNS security (if configured)</li> </ul>"},{"location":"maintainer-guide/admin-services/external-dns/#configuration-and-customization","title":"Configuration and Customization","text":"<p>External-DNS in the ALTERNATIVE platform can be customized to:</p> <ul> <li>Support specific DNS providers</li> <li>Define record naming templates</li> <li>Set up filters for managing specific subsets of services</li> <li>Configure annotation-based customization of DNS records</li> </ul>"},{"location":"maintainer-guide/admin-services/ingress-controller/","title":"Ingress Controller","text":""},{"location":"maintainer-guide/admin-services/ingress-controller/#ingress-controller","title":"Ingress Controller","text":"<p>The platform employs an NGINX-based ingress controller to manage and route ingress traffic to the appropriate internal services. This controller acts as a reverse proxy and load balancer, handling incoming HTTP/S requests and directing them based on URL paths and other criteria. The ingress controller is a critical component for managing external access to the platform's services, providing a secure and efficient gateway for incoming traffic.</p> <p>Each of these administrative services plays a vital role in the overall functionality and security of the ALTERNATIVE platform. They are configured and maintained to ensure optimal performance, security compliance, and seamless integration with other platform components. All of the administrative services are deployed and run on top of Kubernetes.</p>"},{"location":"maintainer-guide/admin-services/istio/","title":"Istio","text":""},{"location":"maintainer-guide/admin-services/istio/#istio-service-mesh","title":"Istio Service Mesh","text":"<p>Istio is a service mesh that provides a comprehensive solution for managing microservices within the ALTERNATIVE platform. It offers advanced features such as traffic management, security, and observability, enhancing the platform's operational capabilities and security posture.</p> <p>The platform leverages Istio's envoy filters to implement custom security policies, such as access control, and authentication. These policies help protect the platform from various security threats and ensure that only authorized traffic is allowed to access the services.</p>"},{"location":"maintainer-guide/admin-services/keycloak/","title":"Keycloak","text":""},{"location":"maintainer-guide/admin-services/keycloak/#overview","title":"Overview","text":"<p>Keycloak serves as the cornerstone of user authentication and authorization within the ALTERNATIVE platform. As an industry-recognized security solution, it provides a comprehensive suite of Identity and Access Management (IAM) features, ensuring the protection of sensitive data and resources.</p>"},{"location":"maintainer-guide/admin-services/keycloak/#key-features","title":"Key Features","text":""},{"location":"maintainer-guide/admin-services/keycloak/#1-single-sign-on-sso","title":"1. Single Sign-On (SSO)","text":"<ul> <li>Seamless authentication across all user-facing applications</li> <li>Streamlined user experience with reduced login prompts</li> <li>Centralized session management for enhanced security</li> </ul>"},{"location":"maintainer-guide/admin-services/keycloak/#2-fine-grained-access-control","title":"2. Fine-Grained Access Control","text":"<ul> <li>Granular permission settings for precise user role management</li> <li>Flexible policy enforcement across various platform components</li> <li>Dynamic adaptation to changing access requirements</li> </ul>"},{"location":"maintainer-guide/admin-services/keycloak/#3-advanced-security-measures","title":"3. Advanced Security Measures","text":"<ul> <li>Two-Factor Authentication (2FA) support</li> <li>Brute-force attack protection</li> <li>Session timeout and idle timeout features</li> </ul>"},{"location":"maintainer-guide/admin-services/keycloak/#4-openid-connect-oidc-implementation","title":"4. OpenID Connect (OIDC) Implementation","text":"<ul> <li>Based on OAuth 2.0 for robust authorization</li> <li>Enhanced interoperability with third-party applications</li> <li>Standardized token-based authentication flow</li> </ul>"},{"location":"maintainer-guide/admin-services/keycloak/#integration-benefits","title":"Integration Benefits","text":"<ol> <li>Centralized User Management: Simplifies administrative tasks and improves security oversight.</li> <li>Scalable Architecture: Easily accommodates growing user bases and evolving access patterns.</li> <li>Compliance Support: Aids in meeting regulatory requirements for data protection and privacy.</li> <li>Customizable User Interfaces: Allows for branded login and registration experiences.</li> </ol>"},{"location":"maintainer-guide/admin-services/keycloak/#technical-implementation","title":"Technical Implementation","text":"<ul> <li>Protocol: OpenID Connect (OIDC) based on OAuth 2.0</li> <li>Token Format: JWT (JSON Web Tokens) for secure information exchange</li> <li>API Security: Token-based authentication for all platform APIs</li> </ul>"},{"location":"maintainer-guide/admin-services/overview/","title":"Administrative Services","text":""},{"location":"maintainer-guide/admin-services/overview/#overview","title":"Overview","text":"<p>The ALTERNATIVE platform incorporates a suite of robust administrative services that form the backbone of its secure, efficient, and scalable infrastructure. These services work in concert to manage user authentication, network routing, SSL/TLS certification, and service mesh capabilities. This section provides an in-depth look at each of these critical components and their roles in maintaining the platform's operational integrity.</p>"},{"location":"maintainer-guide/admin-services/overview/#keycloak","title":"Keycloak","text":"<p>Keycloak serves as the cornerstone of the platform's identity and access management (IAM) system.</p>"},{"location":"maintainer-guide/admin-services/overview/#key-features","title":"Key Features:","text":"<ul> <li>Single Sign-On (SSO): Enables seamless authentication across all platform components.</li> <li>Fine-grained Access Control: Allows for detailed permission settings for users and applications.</li> <li>Multi-factor Authentication (MFA): Enhances security with additional verification layers.</li> <li>OpenID Connect (OIDC) Support: Ensures compatibility with modern authentication protocols.</li> </ul>"},{"location":"maintainer-guide/admin-services/overview/#implementation","title":"Implementation:","text":"<p>Keycloak is deployed within the Kubernetes cluster, integrating tightly with other platform services to provide a unified authentication experience.</p>"},{"location":"maintainer-guide/admin-services/overview/#external-dns","title":"External DNS","text":"<p>External DNS automates the management of DNS records for the platform's services and applications.</p>"},{"location":"maintainer-guide/admin-services/overview/#key-features_1","title":"Key Features:","text":"<ul> <li>Dynamic DNS Updates: Automatically syncs Kubernetes ingress resources with external DNS providers.</li> <li>Multi-provider Support: Compatible with various DNS providers, ensuring flexibility.</li> <li>Kubernetes Native: Operates seamlessly within the Kubernetes ecosystem.</li> </ul>"},{"location":"maintainer-guide/admin-services/overview/#implementation_1","title":"Implementation:","text":"<p>Configured to work with the platform's chosen DNS provider, External DNS maintains accurate and up-to-date DNS records, crucial for service discovery and TLS certificate issuance.</p>"},{"location":"maintainer-guide/admin-services/overview/#cert-manager","title":"Cert Manager","text":"<p>Cert Manager automates the lifecycle management of SSL/TLS certificates within the platform.</p>"},{"location":"maintainer-guide/admin-services/overview/#key-features_2","title":"Key Features:","text":"<ul> <li>Automatic Certificate Issuance and Renewal: Eliminates manual certificate management tasks.</li> <li>Let's Encrypt Integration: Provides free, automated SSL/TLS certificates.</li> <li>Multiple Issuer Support: Allows for flexibility in certificate authorities.</li> </ul>"},{"location":"maintainer-guide/admin-services/overview/#implementation_2","title":"Implementation:","text":"<p>Deployed as a Kubernetes add-on, Cert Manager watches for ingress resources and automatically provisions certificates as needed, ensuring all services maintain valid SSL/TLS encryption.</p>"},{"location":"maintainer-guide/admin-services/overview/#ingress-controller","title":"Ingress Controller","text":"<p>The Ingress Controller, based on NGINX, manages external access to services within the Kubernetes cluster.</p>"},{"location":"maintainer-guide/admin-services/overview/#key-features_3","title":"Key Features:","text":"<ul> <li>Load Balancing: Distributes incoming traffic across multiple service instances.</li> <li>SSL/TLS Termination: Handles encryption/decryption at the edge of the network.</li> <li>Path-based Routing: Directs traffic to different services based on URL paths.</li> </ul>"},{"location":"maintainer-guide/admin-services/overview/#implementation_3","title":"Implementation:","text":"<p>Configured to work in tandem with External DNS and Cert Manager, the Ingress Controller provides a secure and efficient entry point for all external traffic to the platform's services.</p>"},{"location":"maintainer-guide/admin-services/overview/#istio","title":"Istio","text":"<p>Istio is a powerful service mesh that enhances the platform's networking capabilities and security.</p>"},{"location":"maintainer-guide/admin-services/overview/#key-features_4","title":"Key Features:","text":"<ul> <li>Traffic Management: Advanced routing and load balancing features.</li> <li>Security: Mutual TLS encryption between services.</li> <li>Observability: Detailed insights into service-to-service communication.</li> </ul>"},{"location":"maintainer-guide/admin-services/overview/#implementation_4","title":"Implementation:","text":"<p>If implemented, Istio is deployed across the Kubernetes cluster, providing an additional layer of control and visibility over inter-service communication within the platform.</p>"},{"location":"maintainer-guide/admin-services/overview/#integration-and-synergy","title":"Integration and Synergy","text":"<p>These administrative services work together to create a robust, secure, and manageable infrastructure:</p> <ol> <li>Keycloak authenticates users and provides identity information.</li> <li>External DNS ensures services are discoverable.</li> <li>Cert Manager provides SSL/TLS certificates for secure communications.</li> <li>Ingress Controller routes external traffic to the appropriate services.</li> <li>Istio manages and secures inter-service communication.</li> </ol>"},{"location":"maintainer-guide/architecture/cloud-infrastructure/","title":"Cloud Infrastructure","text":""},{"location":"maintainer-guide/architecture/cloud-infrastructure/#overview","title":"Overview","text":"<p>The cloud infrastructure of the ALTERNATIVE data platform is a meticulously designed ecosystem, comprising a range of virtual services and hardware components tailored to support the platform's multifaceted operational requirements. At its core, the infrastructure leverages cutting-edge cloud computing technologies to ensure scalability, resilience, and security, which are paramount for handling the extensive data processing and analysis tasks inherent to the platform.</p>"},{"location":"maintainer-guide/architecture/cloud-infrastructure/#storage-solutions","title":"Storage Solutions","text":"<p>At the heart of this infrastructure lies a suite of storage solutions, including both block storage and object storage services (such as S3). These storage services are designed to offer high durability, scalability, and accessibility, ensuring that data is securely stored and readily available when needed.</p>"},{"location":"maintainer-guide/architecture/cloud-infrastructure/#computational-resources","title":"Computational Resources","text":"<p>In terms of computational resources, the infrastructure boasts an array of Virtual Machines, each equipped with scalable CPU and Memory capabilities. Those are usually deployed by the Kubernetes managed service and act as Kubernetes nodes rather than serving as VMs directly.</p>"},{"location":"maintainer-guide/architecture/cloud-infrastructure/#virtual-networking","title":"Virtual Networking","text":"<p>A key component of the cloud infrastructure is the virtual networking aspect. This includes a comprehensive network architecture that ensures secure, fast, and reliable communication between various services and components within the platform. The virtual networking setup is integral to maintaining the overall performance and integrity of the cloud environment.</p>"},{"location":"maintainer-guide/architecture/cloud-infrastructure/#kubernetes-cluster","title":"Kubernetes Cluster","text":"<p>Central to the infrastructure's capabilities is the Kubernetes cluster. This cluster plays a pivotal role in orchestrating containerized applications, ensuring their seamless deployment, scaling, and management. Kubernetes, renowned for its efficiency and flexibility, enhances the platform's ability to support a wide range of applications and services.</p> <p></p>"},{"location":"maintainer-guide/architecture/kubernetes/","title":"Kubernetes","text":""},{"location":"maintainer-guide/architecture/kubernetes/#overview","title":"Overview","text":"<p>The ALTERNATIVE platform strategically employs a managed Kubernetes service, sourced from the selected cloud provider. This choice ensures a seamless integration between the cloud infrastructure and the Kubernetes environment, creating a cohesive and efficient operational framework.</p>"},{"location":"maintainer-guide/architecture/kubernetes/#abstraction-layer","title":"Abstraction Layer","text":"<p>Kubernetes serves as the foundational target platform for all administrative and user-facing applications within the ALTERNATIVE ecosystem. It functions as a critical abstraction layer, effectively virtualizing and simplifying access to essential resources such as networking, storage, and compute capabilities. This abstraction not only streamlines resource management but also significantly enhances the user and administrator experience by abstracting the complexity of underlying infrastructure.</p>"},{"location":"maintainer-guide/architecture/kubernetes/#scalability","title":"Scalability","text":"<p>One of the key strengths of using Kubernetes in this context is its inherent ability to provide seamless scalability. This feature is crucial for the platform, allowing it to dynamically adjust resource utilization in response to actual demand. Such scalability ensures that the platform can efficiently grow or shrink in resource usage, aligning closely with the varying requirements of the platform's workload.</p>"},{"location":"maintainer-guide/architecture/kubernetes/#cost-effective-resource-utilization","title":"Cost-Effective Resource Utilization","text":"<p>Furthermore, Kubernetes facilitates cost-effective resource utilization. By enabling the platform to scale resources in accordance with real-time demand, it ensures that the infrastructure is neither underutilized nor overburdened. This dynamic scalability not only optimizes performance but also contributes to a more economical use of resources, which is particularly beneficial in a cloud computing environment where resource allocation directly impacts operational costs.</p>"},{"location":"maintainer-guide/architecture/kubernetes/#summary","title":"Summary","text":"<p>In summary, the integration of a managed Kubernetes service is a pivotal aspect of the ALTERNATIVE platform's architecture. It provides a robust, scalable, and cost-effective solution for managing the diverse array of applications and services, underpinning the platform's ability to meet current and future demands efficiently.</p>"},{"location":"maintainer-guide/architecture/networking/","title":"Networking","text":""},{"location":"maintainer-guide/architecture/networking/#overview","title":"Overview","text":"<p>The ALTERNATIVE cloud data platform leverages a sophisticated network architecture designed to support robust, secure, and efficient operations for collaborative research in environmental toxicology. This infrastructure is crucial for facilitating the sharing of large datasets, running complex in-silico models, and enabling seamless collaboration among consortium partners.</p> <p>The network is provisioned by a cloud provider, encompassing a comprehensive suite of virtual network functions:</p> <ol> <li> <p>Virtual Data Center (VDC): A software-defined data center that provides virtualized compute, storage, and networking resources. In the ALTERNATIVE platform, the VDC serves as the foundation for hosting all project-related services and applications.</p> </li> <li> <p>Subnets: Logical subdivisions of the IP network. The platform utilizes multiple subnets to segregate different types of traffic and services, enhancing security and performance. For example, separate subnets are used for the data sharing platform, JupyterHub computational environments, and administrative services.</p> </li> <li> <p>Public and Private IPs: </p> </li> <li>Public IPs: Globally routable addresses that allow resources to be accessed from the internet. In the ALTERNATIVE platform, public IPs are assigned to load balancers and ingress controllers, enabling external access to the web portal and API endpoints.</li> <li> <p>Private IPs: Internal addresses used for communication within the VDC. Most of the platform's services, including databases and backend applications, use private IPs to enhance security by limiting direct internet exposure.</p> </li> <li> <p>Network Gateways: Devices that serve as entry and exit points between networks. The platform employs gateways to manage traffic flow between the internal network and external internet, implementing security policies and routing rules.</p> </li> <li> <p>Load Balancers: Distribute incoming network traffic across multiple servers to ensure no single server becomes overwhelmed, improving the reliability and capacity of applications. In the ALTERNATIVE platform, load balancers are crucial for maintaining high availability of key services like the data portal and JupyterHub.</p> </li> <li> <p>Ingress Controllers: Manage external access to services within the Kubernetes cluster. The platform uses NGINX-based ingress controllers to route HTTP and HTTPS traffic to the appropriate internal services based on defined rules.</p> </li> </ol> <p></p> <p>The network architecture prioritizes:</p> <ol> <li> <p>Security: Implementing multiple layers of protection, including network segmentation, encryption, and access controls, to safeguard sensitive research data on chemical toxicity and biological responses.</p> </li> <li> <p>Scalability: Designed to accommodate growing volumes of omics data and an expanding user base of researchers and regulatory bodies without compromising performance.</p> </li> <li> <p>Flexibility: Adaptable to various research requirements, supporting diverse computational workflows and potential future expansions in the field of environmental toxicology.</p> </li> <li> <p>Performance: Optimized for high-speed data transfer and low-latency operations crucial for running complex in-silico models and analyzing large-scale experimental data.</p> </li> <li> <p>Compliance: Engineered to meet data protection regulations and ethical guidelines pertinent to handling sensitive biological and chemical data.</p> </li> </ol>"},{"location":"maintainer-guide/architecture/networking/#kubernetes-networking","title":"Kubernetes Networking","text":"<p>The ALTERNATIVE platform leverages Kubernetes as its container orchestration system. Kubernetes, often abbreviated as K8s, is an open-source platform designed to automate deploying, scaling, and operating application containers. In the context of the ALTERNATIVE project, Kubernetes plays a crucial role in managing the containerized applications that form the core of the data sharing and analysis infrastructure.</p> <p>Key Kubernetes networking concepts utilized in the ALTERNATIVE platform include:</p> <ol> <li> <p>Pods: The smallest deployable units in Kubernetes. In the ALTERNATIVE platform, pods host individual components of services like data platform, JupyterHub, and various data processing pipelines.</p> </li> <li> <p>Services: Kubernetes abstractions that define a logical set of pods and a policy by which to access them. For example, the web interface and API are exposed as Kubernetes services.</p> </li> <li> <p>NetworkPolicies: These are specifications of how groups of pods are allowed to communicate with each other and other network endpoints. In the ALTERNATIVE platform, NetworkPolicies are used to enforce strict access controls between different components, enhancing security.</p> </li> </ol> <p>The platform utilizes a managed Kubernetes service provided by the cloud provider. This approach offers several advantages:</p> <ul> <li>Simplified cluster management and maintenance</li> <li>Automated scaling and self-healing capabilities</li> <li>Seamless integration with cloud provider's storage and networking services</li> <li>Enhanced security through regular, automated updates</li> </ul>"},{"location":"maintainer-guide/architecture/networking/#container-network-interface-cni","title":"Container Network Interface (CNI)","text":"<p>In the Kubernetes environment, Calico is selected as the Container Network Interface (CNI) plugin. Calico is an open-source networking and network security solution for containers, virtual machines, and native host-based workloads. It's chosen for the ALTERNATIVE platform due to its:</p> <ul> <li>High performance and scalability, crucial for handling large-scale data processing tasks</li> <li>Advanced network policy features, allowing fine-grained control over inter-pod communication</li> <li>Compatibility with a wide range of cloud providers and deployment scenarios</li> </ul> <p>Calico provides a full networking stack for Kubernetes, but in the ALTERNATIVE platform, it's primarily used for its network policy enforcement capabilities, working alongside the cloud provider's native container networking solution.</p>"},{"location":"maintainer-guide/architecture/networking/#application-networking","title":"Application Networking","text":"<p>The networking of applications in the ALTERNATIVE platform emphasizes strong isolation of network segments. This is achieved through:</p> <ol> <li> <p>Kubernetes Namespaces: Logical partitions within the cluster that provide scope for names and allow for fine-grained access control. Separate namespaces are used for different components of the platform, such as CKAN, JupyterHub, and supporting services like Keycloak for authentication.</p> </li> <li> <p>Pod Security Policies: These control security-sensitive aspects of pod specification, restricting the capabilities of containers and enhancing overall system security.</p> </li> </ol> <p>Traffic ingress is managed by an NGINX-based Ingress controller. This controller:</p> <ul> <li>Routes external HTTP and HTTPS traffic to the appropriate services within the cluster</li> <li>Terminates SSL/TLS connections</li> <li>Provides features like URL rewriting, rate limiting, and authentication</li> </ul>"},{"location":"maintainer-guide/architecture/networking/#security","title":"Security","text":"<p>Security is a paramount concern in the ALTERNATIVE platform's network architecture, particularly given the sensitive nature of the toxicological and biological data being processed. Key security measures include:</p> <ol> <li> <p>TLS Encryption: All external traffic (e.g., users accessing the web portal) is encrypted using TLS protocols.</p> </li> <li> <p>cert-manager: This Kubernetes add-on automates the management and issuance of TLS certificates. In the ALTERNATIVE platform, cert-manager is configured to automatically obtain and renew certificates from Let's Encrypt, ensuring that all services are always protected by valid SSL/TLS certificates.</p> </li> <li> <p>Network Policies: Kubernetes NetworkPolicies are extensively used to control traffic flow between different components of the platform, implementing the principle of least privilege.</p> </li> <li> <p>Secure API Gateway: All external API access is routed through a secure API gateway that handles authentication, rate limiting, and monitoring.</p> </li> </ol>"},{"location":"maintainer-guide/architecture/networking/#dns-configuration","title":"DNS Configuration","text":"<p>DNS (Domain Name System) configuration is a critical aspect of the ALTERNATIVE platform's network setup. It's managed by the external-dns Kubernetes addon, which automates the creation and management of DNS records.</p> <p>Key features of the DNS setup include:</p> <ol> <li> <p>Automated DNS Management: external-dns watches for new or deleted services and ingresses in the Kubernetes cluster and automatically creates, updates, or deletes corresponding DNS records in the configured DNS provider.</p> </li> <li> <p>Support for Multiple DNS Providers: While the ALTERNATIVE platform primarily uses the cloud provider's DNS service, external-dns supports a wide range of DNS providers, offering flexibility for future changes or multi-cloud deployments.</p> </li> <li> <p>Integration with Ingress Controllers: external-dns works seamlessly with the NGINX ingress controller, automatically creating DNS entries for newly deployed services exposed via ingress rules.</p> </li> <li> <p>TTL Management: Time-to-Live (TTL) values for DNS records are carefully managed to balance between rapid propagation of changes and reducing load on DNS servers.</p> </li> <li> <p>Alias Record Support: For services that benefit from cloud provider-specific routing optimizations, external-dns can create alias records (like AWS Route53 Alias records) instead of standard A records.</p> </li> </ol> <p>This automated DNS management significantly reduces manual effort and the potential for errors, ensuring that all services within the ALTERNATIVE platform are consistently accessible via their designated hostnames.</p>"},{"location":"maintainer-guide/architecture/networking/#conclusion","title":"Conclusion","text":"<p>The network architecture of the ALTERNATIVE cloud data platform is a sophisticated system designed to meet the unique requirements of collaborative research in environmental toxicology. By leveraging advanced technologies like Kubernetes, implementing robust security measures, and automating key processes like DNS management, the platform provides a secure, scalable, and efficient infrastructure for data sharing and analysis.</p> <p>This architecture not only meets the current demands of the ALTERNATIVE project but is also designed to be adaptable and scalable for future needs. As the field of environmental toxicology evolves and new computational methods emerge, the platform's networking infrastructure is well-positioned to support ongoing research and collaboration among consortium partners and the wider scientific community.</p>"},{"location":"maintainer-guide/architecture/overview/","title":"Architecture Introduction","text":""},{"location":"maintainer-guide/architecture/overview/#overview","title":"Overview","text":"<p>This section provides an in-depth overview of the ALTERNATIVE cloud data platform architecture. It is designed to offer a comprehensive understanding of the project's structure, components, and their intricate interactions. The architecture has been meticulously crafted to be modular, scalable, and extensible, facilitating seamless integration of new features and components as the project evolves.</p>"},{"location":"maintainer-guide/architecture/overview/#key-components","title":"Key Components","text":"<p>The ALTERNATIVE platform comprises several key components:</p> <ol> <li> <p>Cloud Infrastructure: Serves as the foundation, providing virtual services and hardware components essential for the platform's operation.</p> </li> <li> <p>Kubernetes: Acts as the orchestration layer, managing containerized applications and services across the cloud infrastructure.</p> </li> <li> <p>Administrative Services: Including Keycloak for user management, external-dns for DNS management, cert-manager for SSL/TLS certificate management, and an ingress-controller for routing external traffic.</p> </li> <li> <p>Storage Solutions: Utilizing a combination of S3 object storage, block storage, and NFS for various data storage needs.</p> </li> <li> <p>User Services: Primarily Data sharing and JupyterHub for providing an integrated development environment.</p> </li> <li> <p>APIs: Facilitating communication between different components and enabling external integrations.</p> </li> </ol>"},{"location":"maintainer-guide/architecture/overview/#architectural-highlights","title":"Architectural Highlights","text":"<ul> <li>Scalability: The architecture is designed to scale horizontally, allowing for increased load handling as the platform grows.</li> <li>Security: Implements robust security measures, including SSL/TLS encryption, IAM through Keycloak, and fine-grained access controls.</li> <li>Flexibility: The modular design allows for easy updates and additions to individual components without affecting the entire system.</li> <li>Collaboration: Supports multi-user environments and facilitates seamless data sharing and collaborative work.</li> <li>Extensibility: Employs a modular extension mechanism to incorporate custom functionality tailored to the specific requirements of the ALTERNATIVE project.</li> </ul>"},{"location":"maintainer-guide/architecture/overview/#purpose-and-goals","title":"Purpose and Goals","text":"<p>The ALTERNATIVE platform architecture is specifically designed to:</p> <ol> <li>Facilitate efficient data exchange between consortium partners</li> <li>Host and manage large-scale omics datasets</li> <li>Provide a robust environment for in-silico machine learning models</li> <li>Offer APIs for external users and systems to access platform resources</li> <li>Ensure data security and user privacy while promoting collaboration</li> </ol> <p>In the following sections, we will delve deeper into each component, exploring their roles, interactions, and the technologies that power them. This comprehensive view will provide a solid foundation for understanding the ALTERNATIVE platform's capabilities and potential for future enhancements.</p>"},{"location":"maintainer-guide/architecture/storage/","title":"Storage","text":""},{"location":"maintainer-guide/architecture/storage/#overview","title":"Overview","text":"<p>The storage architecture within the ALTERNATIVE platform is designed to provide robust, scalable, and persistent data storage solutions. This is achieved by leveraging Kubernetes' powerful storage abstractions: <code>StorageClass</code>, <code>Persistent Volume Claim (PVC)</code>, and <code>Volume</code>. These abstractions decouple applications from the underlying storage infrastructure, enhancing flexibility and scalability.</p> <p> Figure 2: ALTERNATIVE cloud storage architecture</p>"},{"location":"maintainer-guide/architecture/storage/#dynamic-allocation-of-data-volumes","title":"Dynamic Allocation of Data Volumes","text":"<p>The platform utilizes Kubernetes' dynamic provisioning capabilities to allocate data volumes as needed. This approach ensures efficient use of storage resources by creating volumes on-the-fly to meet application demands, eliminating the need for pre-provisioning storage, optimizing resource allocation, and reducing overhead.</p>"},{"location":"maintainer-guide/architecture/storage/#persistence-and-reattachment","title":"Persistence and Reattachment","text":"<p>A key feature of the platform's storage strategy is data persistence, crucial for maintaining data integrity and continuity. Persistent data volumes remain intact even if the associated application is terminated or fails. These volumes can be seamlessly reattached to other instances of the application, ensuring data continuity and minimizing downtime.</p>"},{"location":"maintainer-guide/architecture/storage/#delegation-of-scalability-and-recovery","title":"Delegation of Scalability and Recovery","text":"<p>The platform delegates scalability, recovery, and persistence responsibilities to the cloud provider. By leveraging the cloud provider's robust infrastructure and services, the platform benefits from high scalability and efficient recovery mechanisms. This delegation allows the platform to focus on its core functionalities while relying on the cloud provider for maintaining storage resilience and scalability.</p>"},{"location":"maintainer-guide/architecture/storage/#types-of-storage-used","title":"Types of Storage Used","text":"<ul> <li>Block Storage: The platform employs block storage as the primary file system for applications. This type of storage is ideal for scenarios where performance and low latency are critical, providing high-speed access to data suitable for various workloads.</li> <li>S3 Storage: For the Data storing and sharing component of the platform, S3 storage is utilized. Known for its scalability and durability, S3 is an object storage service offering a simple and cost-effective solution for storing and retrieving large amounts of data, aligning well with platform's design goal of handling large datasets.</li> </ul>"},{"location":"maintainer-guide/architecture/storage/#s3-storage","title":"S3 Storage","text":"<p>The ALTERNATIVE platform has specific storage requirements due to the potentially large size of datasets, particularly those containing omics data. To manage these datasets effectively, the platform utilizes S3-compatible storage, which is well-suited for handling high volumes of data.</p>"},{"location":"maintainer-guide/architecture/storage/#scalability-and-cost-effectiveness-of-s3","title":"Scalability and Cost-Effectiveness of S3","text":"<p>S3 storage is chosen for its exceptional scalability and cost-effectiveness. It handles vast amounts of data, scaling seamlessly with the platform's storage needs. This scalability ensures that as the volume of omics data grows, the storage infrastructure can grow correspondingly without significant challenges. Additionally, S3's pricing model, which involves paying only for the storage used, makes it a cost-effective solution for managing large datasets.</p>"},{"location":"maintainer-guide/architecture/storage/#custom-platform-integration-with-s3","title":"Custom Platform Integration with S3","text":"<p>Integration with S3-compatible storage is achieved through a custom extension, enabling efficient interaction with the storage backend. This facilitates optimized data storage and retrieval processes, ensuring both performance and reliability.</p>"},{"location":"maintainer-guide/architecture/storage/#direct-interface-with-s3","title":"Direct Interface with S3","text":"<p>In the ALTERNATIVE platform, the data management component interfaces directly with S3-compatible storage, bypassing integration through the Kubernetes layer. This direct interaction streamlines data flow, minimizing complexity and reducing potential bottlenecks associated with additional integration layers.</p>"},{"location":"maintainer-guide/architecture/storage/#s3-storage-service-and-minio-library","title":"S3 Storage Service and Minio Library","text":"<p>The S3 storage service provided by the cloud provider utilizes the MinIO library, an open-source object storage server fully compatible with Amazon S3 APIs. MinIO enhances the platform's ability to interact with S3 storage efficiently and securely.</p>"},{"location":"maintainer-guide/architecture/storage/#api-based-data-exposure","title":"API-based Data Exposure","text":"<p>The platform does not expose the underlying S3-compatible storage directly to end users. Instead, it provides a dedicated set of APIs for data access, ensuring controlled and secure interaction with stored data. This approach enhances security and enables the implementation of additional features and access controls, governing how data is retrieved and utilized.</p> <p>In summary, the use of S3-compatible storage within the ALTERNATIVE platform addresses the specific requirements of large-scale omics data management. The custom integration, combined with a direct interface approach and the use of the MinIO library, ensures efficient handling of large datasets while maintaining both cost-effectiveness and security.</p>"},{"location":"maintainer-guide/architecture/storage/#block-storage","title":"Block Storage","text":"<p>In the ALTERNATIVE platform, block storage is the main type of data storage utilized by applications. This section details the implementation and management of block storage within the platform's architecture.</p>"},{"location":"maintainer-guide/architecture/storage/#exposure-as-file-system","title":"Exposure as File System","text":"<p>Block storage is exposed to applications and services as a file system, allowing developers to interact with the storage using standard file operations. This method simplifies the process of reading from and writing to the storage medium, making it highly accessible for a variety of applications.</p>"},{"location":"maintainer-guide/architecture/storage/#underlying-storage-system","title":"Underlying Storage System","text":"<p>Block storage systems are typically disk-based, utilizing either Hard Disk Drives (HDD) or Solid State Drives (SSD). The choice between HDD and SSD depends on a trade-off between cost and performance. SSDs, with their faster data access speeds, are well-suited for high-performance requirements, while HDDs are used for cost-effective storage solutions where speed is less critical.</p>"},{"location":"maintainer-guide/architecture/storage/#consumption-via-kubernetes-apis","title":"Consumption via Kubernetes APIs","text":"<p>Block storage within the platform is consumed through Kubernetes APIs using volumes, Persistent Volume Claims (PVCs), and Storage Classes. This integration ensures seamless and efficient management of storage resources within the platform's ecosystem. The platform offers flexibility in defining different storage classes tailored to meet varying performance requirements, allowing customization based on the specific needs of different applications or services.</p>"},{"location":"maintainer-guide/architecture/storage/#decoupling-of-applications-from-storage","title":"Decoupling of Applications from Storage","text":"<p>Using block storage in this manner decouples applications from the physical storage, enhancing flexibility in scenarios such as backup, disaster recovery, and migration. By abstracting the storage layer, applications can be easily moved, backed up, or restored without managing the complexities of the underlying storage infrastructure, significantly improving the platform's resilience and agility in handling data.</p>"},{"location":"maintainer-guide/architecture/storage/#nfs","title":"NFS","text":"<p>The platform deploys an NFS (Network File System) server to provide an additional storage class for some applications. For instance, JupyterHub users require a shared data folder for collaboration, necessitating a storage class with mode <code>ReadWriteMany</code>. Since the cloud provider only offers block storage classes of type <code>ReadWriteOnce</code>, the NFS server is used to expose such a volume.</p>"},{"location":"maintainer-guide/architecture/storage/#alternative-data-management-storage","title":"ALTERNATIVE Data Management Storage","text":"<p>The Platform uses several types of storage: - Database: A relational database (Postgres) is used to store the application state, including metadata and user data. - S3: User-defined datasets are mapped to S3 buckets. - Block Storage: Regular files, such as configuration files needed by the Platform, use block storage.</p>"},{"location":"maintainer-guide/architecture/storage/#jupyterhub-storage","title":"JupyterHub Storage","text":"<p> Figure 3: Diagram of Jupyter Storage</p> <p>JupyterHub has more complex storage requirements compared to other ALTERNATIVE services.</p>"},{"location":"maintainer-guide/architecture/storage/#multi-user-capability","title":"Multi-user Capability","text":"<p>Each user is dynamically allocated a Kubernetes POD containing the Jupyter kernel and runtime. The POD is ephemeral, but its associated data must persist to a user volume (10 GB). Each user is allocated this volume upon first login, and it remains after session timeout or logout. When the POD is recreated, it attaches to the same volume, preserving the user's state from the last session.</p>"},{"location":"maintainer-guide/architecture/storage/#shared-folder","title":"Shared Folder","text":"<p>Users can share data directly in Jupyter via a shared folder mapped to a static volume (20 GB) with mode <code>ReadWriteMany</code>. Multiple PODs must be able to attach and write to this volume simultaneously, provided by the NFS server.</p>"},{"location":"maintainer-guide/architecture/storage/#accessing-alternative-platform-data-from-jupyter","title":"Accessing ALTERNATIVE platform data from Jupyter","text":"<p>A custom Python library, <code>alternative-lib</code>, simplifies access to ALTERNATIVE/S3 data from Jupyter. Hosted on GitHub as an open-source project, it enhances user experience by providing seamless data access.</p>"},{"location":"maintainer-guide/architecture/storage/#databases","title":"Databases","text":"<p>A Postgres database, deployed within the Kubernetes plane, is used by the Platform and Keycloak services, leveraging block storage.</p>"},{"location":"maintainer-guide/deployment/ai-ml-api/","title":"AI/ML API","text":""},{"location":"maintainer-guide/deployment/ai-ml-api/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding with the installation or deployment of the AI/ML API, ensure you have the following prerequisites:</p> <ul> <li>Python 3.9</li> <li>Anaconda</li> <li><code>PipelineAlternative_clinicaldata</code> data</li> <li>PBPK (Physiologically Based Pharmacokinetic) data</li> <li>cddd (Compound Database for Drug Discovery) data</li> </ul>"},{"location":"maintainer-guide/deployment/ai-ml-api/#installation","title":"Installation","text":""},{"location":"maintainer-guide/deployment/ai-ml-api/#clone-the-repository","title":"Clone the Repository","text":"<pre><code>git clone git@github.com:ALTERNATIVE-EU/ai-ml-api.git\n</code></pre>"},{"location":"maintainer-guide/deployment/ai-ml-api/#prepare-data-directories","title":"Prepare Data Directories","text":"<p>Place the required data directories in their respective locations:</p> <ul> <li><code>PipelineAlternative_clinicaldata</code> <code>models</code> directory</li> <li><code>cddd</code> directories inside <code>PipelineAlternative_clinicaldata</code></li> <li><code>PBPK</code> directory inside <code>models</code></li> </ul> <p>Copy the contents of the <code>patches</code> directory into the <code>models</code> directory:</p> <pre><code>cp -r patches/* ./models/\n</code></pre>"},{"location":"maintainer-guide/deployment/ai-ml-api/#set-up-virtual-environments","title":"Set up Virtual Environments","text":""},{"location":"maintainer-guide/deployment/ai-ml-api/#cddd-environment","title":"cddd Environment","text":"<p>Create and activate a virtual environment for <code>cddd</code>:</p> <pre><code>cd cddd\nconda env create -f environment.yml\nconda activate cddd\npip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.10.0-cp36-cp36m-linux_x86_64.whl\npip install -e .\nconda deactivate\n</code></pre>"},{"location":"maintainer-guide/deployment/ai-ml-api/#alternative-environment","title":"Alternative Environment","text":"<p>Create and activate the <code>alternative</code> virtual environment:</p> <pre><code>conda create -n alternative python=3.9\nconda activate alternative\n</code></pre> <p>Install dependencies from the requirements file:</p> <pre><code>pip install -f requirements.txt\n</code></pre>"},{"location":"maintainer-guide/deployment/ai-ml-api/#start-the-server","title":"Start the Server","text":"<p>Launch the API server by running:</p> <pre><code>python3 app.py\n</code></pre>"},{"location":"maintainer-guide/deployment/ai-ml-api/#installation-with-docker","title":"Installation with Docker","text":"<p>Build and run the AI/ML API container using Docker:</p> <ol> <li> <p>Build the image:</p> <pre><code>docker build -t ai-ml-api .\n</code></pre> </li> <li> <p>Run the container:</p> <pre><code>docker run -p 5000:5000 ai-ml-api\n</code></pre> </li> </ol>"},{"location":"maintainer-guide/deployment/ai-ml-api/#deployment-in-kubernetes","title":"Deployment in Kubernetes","text":"<p>Update and apply the deployment files located in the <code>deployment/kubernetes</code> directory:</p> <pre><code>kubectl apply -f deployment/kubernetes\n</code></pre> <p>For authentication, use Istio with a custom Envoy filter. Follow the instructions provided in the envoy-filter documentation.</p>"},{"location":"maintainer-guide/deployment/ai-ml-api/#api-usage","title":"API Usage","text":""},{"location":"maintainer-guide/deployment/ai-ml-api/#endpoints","title":"Endpoints","text":""},{"location":"maintainer-guide/deployment/ai-ml-api/#ml-evaluation","title":"ML Evaluation","text":"<pre><code>curl -X POST -H \"Content-Type: application/json\" -d '{\"smiles\": \"c1ccccc1O\"}' https://ai-ml-api.platform.alternative-project.eu/clinicaldata/ml/evaluate -o results.csv\n</code></pre>"},{"location":"maintainer-guide/deployment/ai-ml-api/#ai-evaluation","title":"AI Evaluation","text":"<pre><code>curl -X POST -H \"Content-Type: application/json\" -d '{\"smiles\": \"c1ccccc1O\"}' https://ai-ml-api.platform.alternative-project.eu/clinicaldata/ai/evaluate\n</code></pre>"},{"location":"maintainer-guide/deployment/ai-ml-api/#aop-alternate-operating-procedure-evaluation","title":"AOP (Alternate Operating Procedure) Evaluation","text":"<pre><code>curl -X POST -H \"Content-Type: application/json\" -d '{\"smiles\": \"c1ccccc1O\"}' https://ai-ml-api.platform.alternative-project.eu/clinicaldata/aop/evaluate\n</code></pre>"},{"location":"maintainer-guide/deployment/ai-ml-api/#doxorubicin-pbpk-model","title":"Doxorubicin PBPK Model","text":"<pre><code>curl -X POST https://ai-ml-api.platform.alternative-project.eu/pbpk/doxorubicin      -H \"Content-Type: application/json\"      -d '{\"dose_mg\": 60, \"age\": 50, \"weight\": 70, \"height\": 190}'\n</code></pre>"},{"location":"maintainer-guide/deployment/ai-ml-api/#httk-human-toxicology-toolkit-model","title":"HTTK (Human Toxicology Toolkit) Model","text":"<pre><code>curl -X POST https://ai-ml-api.platform.alternative-project.eu/pbpk/httk -H \"Content-Type: application/json\"      -d '{\"chem_name\": \"Bisphenol A\", \"species\": \"human\", \"daily_dose\": 1, \"doses_per_day\": 1, \"days\": 15}'\n</code></pre>"},{"location":"maintainer-guide/deployment/ai-ml-api/#isalive-check","title":"IsAlive Check","text":"<p>Verify if the server is alive:</p> <pre><code>curl https://ai-ml-api.platform.alternative-project.eu/isalive\n</code></pre>"},{"location":"maintainer-guide/deployment/ai-ml-api/#testing","title":"Testing","text":"<p>To run unit tests, use the following command:</p> <pre><code>python -m unittest app_test.py\n</code></pre>"},{"location":"maintainer-guide/deployment/envoy-filter/","title":"Alternative Envoy Filter","text":""},{"location":"maintainer-guide/deployment/envoy-filter/#overview","title":"Overview","text":"<p>This document describes the implementation and usage of a alternative Envoy filter designed for use in an Istio service mesh. The filter, written in Go, validates JWT tokens by checking if the token's ID has been revoked, as recorded in a database, and by verifying the token's signature. This ensures that only valid and active tokens are allowed, enhancing the security within the service mesh.</p>"},{"location":"maintainer-guide/deployment/envoy-filter/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes cluster</li> <li>Service with deployment which will be protected by the filter</li> <li>Istio installed on the cluster</li> <li>A JWT token issuer, such as Keycloak, to generate JWT tokens</li> <li>Docker</li> <li>PostgreSQL database</li> </ul>"},{"location":"maintainer-guide/deployment/envoy-filter/#architecture","title":"Architecture","text":"<p>The alternative Envoy filter is registered as an HTTP filter in the Envoy proxy. The filter is written in Go. The filter is deployed as a sidecar container in the same pod as the service, and it intercepts all incoming requests to the service. The filter validates the JWT token signature and checks if the token ID has been revoked. If the token is valid, the request is forwarded to the service. If the token is invalid, the request is rejected. The filter also caches the token's revocation status to reduce the number of database queries, and the also caches the public key used to verify the token's signature. </p>"},{"location":"maintainer-guide/deployment/envoy-filter/#envoy-and-istio-integration","title":"Envoy and Istio Integration","text":"<p>The alternative filter is integrated with Envoy and Istio as follows:</p> <ol> <li> <p>The alternative filter is built as a shared object file (.so) and is registered as an HTTP filter in the Envoy proxy. The filter is deployed as a sidecar container in the same pod as the service.</p> </li> <li> <p>The filter intercepts all incoming requests to the service. The filter validates the JWT token signature and checks if the token ID has been revoked. If the token is valid, the request is forwarded to the service. If the token is invalid, the request is rejected.</p> </li> <li> <p>The filter caches the token's revocation status to reduce the number of database queries, and also caches the public key used to verify the token's signature.</p> </li> </ol>"},{"location":"maintainer-guide/deployment/envoy-filter/#database-interaction","title":"Database Interaction","text":"<p>The filter interacts with a database to check if the token ID has been revoked. The filter uses a PostgreSQL database to store the token IDs that have been revoked. The filter queries the database to check if the token ID is present in the database, and if the token ID is present, the filter rejects the request. The filter caches the token's revocation status to reduce the number of database queries.</p>"},{"location":"maintainer-guide/deployment/envoy-filter/#getting-started","title":"Getting Started","text":""},{"location":"maintainer-guide/deployment/envoy-filter/#installation","title":"Installation","text":"<ol> <li>Clone the repository:</li> </ol> <p><pre><code>git clone git@github.com:ALTERNATIVE-EU/auth-envoy-filter.git\n</code></pre> 1. Navigate to the <code>envoy-filter</code> directory:</p> <pre><code>cd envoy-filter\n</code></pre> <ol> <li> <p>Use <code>envoy-fitlter/database/schema.sql</code> to create the database schema in your PostgreSQL database.</p> </li> <li> <p>Compile the envoy filter:</p> </li> </ol> <p><code>bash    docker compose run --rm go_plugin_compile</code></p> <ol> <li>Create a PVC which will store the compiled envoy filter:</li> </ol> <p><code>bash    kubectl apply -f manifests/pvc.yaml</code></p> <ol> <li>Add volume to the service's deployment:</li> </ol> <p><code>bash    kubectl patch deployment &lt;depoyment-name&gt; -n &lt;namespace --type=json -p='[{\"op\": \"add\", \"path\": \"/spec/template/spec/volumes/-\", \"value\": {\"name\": \"jwt-revocation-validation\", \"persistentVolumeClaim\": {\"claimName\": \"jwt-revocation-validation\"}}}]'</code></p> <ol> <li>Add volumeMount to the sidecar container:</li> </ol> <p><code>bash    kubectl patch deployment &lt;depoyment-name&gt; -n &lt;namespace&gt; --type=json -p='[{\"op\": \"add\", \"path\": \"/spec/template/spec/containers/1/volumeMounts/-\", \"value\": {\"mountPath\": \"/etc/istio/jwt-revocation-validation\", \"name\": \"jwt-revocation-validation\"}}]'</code></p> <ol> <li>Copy the envoy filter to the <code>revocation_filter</code> directory:</li> </ol> <p><code>bash     kubectl cp -n &lt;namespace&gt; revocation_filter/revocation_filter.so -c istio-proxy $(kubectl get pod -n &lt;namespace&gt; -l app=&lt;app-name&gt; -o custom-columns=NAME:.metadata.name --no-headers):/etc/istio/jwt-revocation-validation/revocation_filter.so</code></p> <ol> <li> <p>Update the configuration in the <code>envoy-filter/manifests/envoyfilter.yaml</code> file to match your environment. Make sure to update <code>jwks_url</code> and <code>data_source_name</code> fields.</p> </li> <li> <p>Apply the envoy filter manifest:     <pre><code>kubectl apply -f manifests/envoyfilter.yaml\n</code></pre></p> </li> </ol>"},{"location":"maintainer-guide/deployment/envoy-filter/#configuration","title":"Configuration","text":"<p>The alternative filter is configured using the <code>envoy-filter/manifests/envoyfilter.yaml</code> file. The configuration options are as follows:</p> <ul> <li><code>jwks_url</code>: The URL of the JWKS endpoint that contains the public key used to verify the JWT token signature.</li> <li><code>data_source_name</code>: The data source connection string for the PostgreSQL database.</li> <li><code>public_key_cache_ttl</code>: The time-to-live (TTL) for the public key cache.</li> <li><code>revocation_status_cache_ttl</code>: The time-to-live (TTL) for the revocation status cache.</li> </ul>"},{"location":"maintainer-guide/deployment/envoy-filter/#usage","title":"Usage","text":"<ol> <li> <p>Generate a JWT token using a JWT token issuer, such as Keycloak.</p> </li> <li> <p>Send a request to the service with the JWT token in the <code>Authorization</code> header:</p> </li> </ol> <p><code>bash    curl -H \"Authorization: Bearer &lt;JWT token&gt;\" https://ai-ml-api.platform.alternative-project.eu/</code> </p> <ol> <li> <p>The filter will validate the JWT token and forward the request to the service if the token is valid. If the token is invalid, the filter will reject the request.</p> </li> <li> <p>Add the token to the revocation list:</p> </li> </ol> <p><code>bash    psql -h &lt;database-host&gt; -U &lt;database-username&gt; -d &lt;database-name&gt; -c \"INSERT INTO revoked_tokens (token_id) VALUES ('&lt;token-id&gt;');\"</code></p> <ol> <li>Send a request to the service with the JWT token in the <code>Authorization</code> header:</li> </ol> <p><code>bash     curl -H \"Authorization: Bearer &lt;JWT token&gt;\" https://ai-ml-api.platform.alternative-project.eu/</code></p> <ol> <li>The filter will reject the request because the token has been revoked.</li> </ol>"},{"location":"maintainer-guide/deployment/envoy-filter/#trigger-refresh-cache","title":"Trigger refresh cache","text":"<p>Send a request with header \"X-Refresh-Cache: true\", the envoy filter will trigger a manual refresh of the revocation status caches.</p>"},{"location":"maintainer-guide/deployment/platform/","title":"Install ALTERNATIVE CKAN Environment on K8S","text":""},{"location":"maintainer-guide/deployment/platform/#overview","title":"Overview","text":"<p>The Alternative platform is a comprehensive cloud-based solution designed to facilitate data sharing, analysis, and collaboration in scientific research. This guide provides detailed instructions for installing and configuring the platform's core components.</p>"},{"location":"maintainer-guide/deployment/platform/#platform-architecture","title":"Platform Architecture","text":"<p>The Alternative platform consists of several key building blocks:</p> <ol> <li>CKAN (Comprehensive Knowledge Archive Network): For data management and sharing</li> <li>Keycloak: For identity and access management</li> <li>JupyterHub: For interactive data analysis and development</li> <li>Kubernetes: For orchestrating and managing containerized applications</li> <li>PostgreSQL: As the primary database for CKAN and Keycloak</li> <li>Cloud Storage (S3): For storing large datasets</li> <li>AI/ML APIs: For integrating machine learning models</li> </ol> <p>These components interact to provide a seamless experience for researchers, data scientists, and administrators.</p>"},{"location":"maintainer-guide/deployment/platform/#requirements","title":"Requirements","text":"<p>You will need a cluster with:</p> <ul> <li>Ingress controller configured</li> <li>Cert-Manager configured</li> <li>Domain name resolving to ingress-controller's service external IP</li> <li>GCP bucket and credentials json file for that project</li> <li>Kubeconfig file for interaction with the cluster (set environment variable <code>KUBECONFIG</code> to point to it)</li> </ul> <p>You also need to clone platform-deployment repository and all the CKAN extension ones.</p>"},{"location":"maintainer-guide/deployment/platform/#create-certificate","title":"Create Certificate","text":"<ol> <li>Navigate to the <code>platform-deployment</code> repository <pre><code>cd platform-deployment\n</code></pre></li> <li>Update <code>dnsNames</code> and <code>issuerRef</code> params in <code>deployment/manifest/certificate.yaml</code></li> <li>Create the certificate resource <pre><code>kubectl apply -f ./deployment/manifests/certificate.yaml\n</code></pre></li> </ol>"},{"location":"maintainer-guide/deployment/platform/#create-ingress","title":"Create Ingress","text":"<ol> <li>Update <code>tls</code> and <code>rules</code> params in <code>deployment/manifest/ingress.yaml</code></li> <li>Create the ingress resource <pre><code>kubectl apply -f ./deployment/manifests/ingress.yaml\n</code></pre></li> </ol>"},{"location":"maintainer-guide/deployment/platform/#install-keycloak","title":"Install Keycloak","text":"<ol> <li>Install helm chart <pre><code>helm install -f ./deployment/charts/keycloak/values.yaml keycloak ./deployment/charts/keycloak/ --namespace default\n</code></pre></li> <li>Get admin credentials (username is <code>user</code>) <pre><code>kubectl get secret keycloak -o jsonpath='{.data.admin-password}'|base64 --decode\n</code></pre></li> <li>Create alternative realm from json file <code>deployment/charts/keycloak/realms/alternative-realm.json</code></li> <li>Update URL parameters in <code>ckan-backend</code>, <code>ckan-frontend</code> and <code>jupyterhub</code> clients (add jupyterhub URL)</li> <li>Generate new client credentials secret for <code>ckan-backend</code> and <code>jupyterhub</code> clients</li> <li>Configure realm email settings</li> <li>Enable <code>Forgot password</code> functionality</li> </ol>"},{"location":"maintainer-guide/deployment/platform/#restore-keycloak-backup","title":"Restore Keycloak Backup","text":"<ol> <li> <p>Install the helm chart and wait for the pods to be ready and running <pre><code>helm install -f ./deployment/charts/keycloak/values.yaml keycloak ./deployment/charts/keycloak/ --namespace default\n</code></pre></p> </li> <li> <p>Copy the <code>.dump</code> file to the Keycloak DB pod <pre><code>kubectl cp keycloak.dump keycloak-postgresql-0:/tmp/backup.dump\n</code></pre></p> </li> <li> <p>Remove Keycloak pod <pre><code>kubectl scale statefulsets keycloak --replicas=0\n</code></pre></p> </li> <li> <p>Get PostgreSQL password <pre><code>kubectl get secret keycloak-postgresql -o jsonpath='{.data.password}' | base64 --decode\n</code></pre></p> </li> <li> <p>Enter Keycloak DB pod <pre><code>kubectl exec -it keycloak-postgresql-0 /bin/bash\n</code></pre></p> </li> <li> <p>Set environment variables (replace <code>pass</code> with the PostgreSQL password) <pre><code>export PGDATABASE=bitnami_keycloak\nexport PGUSER=bn_keycloak\nexport PGPASSWORD=pass\n</code></pre></p> </li> <li> <p>Recreate the DB from the backup file <pre><code>dropdb -f $PGDATABASE\ncreatedb $PGDATABASE\npg_restore -d $PGDATABASE /tmp/backup.dump\n</code></pre></p> </li> <li> <p>Restoring from the backup recreates the main user of Keycloak so the password in the secret will no longer be correct, to fix that:</p> </li> <li>Start PostgreSQL console with <code>psql</code></li> <li>Get the user ID of user with username <code>user</code> <pre><code>select id from user_entity where \"username\"='user';\n</code></pre></li> <li>Run these queries (replace <code>usr_id</code>) <pre><code>delete from credential where \"user_id\"='usr_id';\ndelete from user_role_mapping where \"user_id\"='usr_id';\ndelete from user_entity where \"id\"='usr_id';\n</code></pre></li> <li> <p>Exit DB pod, delete it with the below command and wait for it to be recreated, ready and running again <pre><code>kubectl delete pod keycloak-postgresql-0\n</code></pre></p> </li> <li> <p>Recreate Keycloak pod <pre><code>kubectl scale statefulsets keycloak --replicas=1\n</code></pre></p> </li> <li> <p>Enter Keycloak with username <code>user</code> and password from this command <code>kubectl get secret keycloak -o jsonpath='{.data.admin-password}' | base64 --decode</code> and check if everything got recovered</p> </li> </ol>"},{"location":"maintainer-guide/deployment/platform/#build-alternative-ckan-docker-image","title":"Build ALTERNATIVE CKAN Docker Image","text":"<ol> <li>Update configs in <code>ckan-alternative-theme/keycloak_auth-config</code> and <code>ckan-alternative-theme/cloudstorage-config</code></li> <li>Copy the CKAN extensions into <code>ckan-alternative-theme</code></li> <li>Build the image <pre><code>docker build -f ./ckan-alternative-theme/AlternativeCKAN ./ckan-alternative-theme -t gcr.io/alternative-363010/ckan-alternative\n</code></pre></li> <li>Upload the image to the registry <pre><code>docker push gcr.io/alternative-363010/ckan-alternative\n</code></pre></li> </ol>"},{"location":"maintainer-guide/deployment/platform/#install-ckan","title":"Install CKAN","text":"<ol> <li>Add chart repo <pre><code>helm repo add keitaro-charts https://keitaro-charts.storage.googleapis.com\n</code></pre></li> <li>Update <code>deployment/manifests/ckan_values.yaml</code></li> <li>Install helm chart <pre><code>helm install -f ./deployment/manifests/ckan_values.yaml ckan keitaro-charts/ckan\n</code></pre></li> <li>Wait for the ckan pod to become ready</li> </ol>"},{"location":"maintainer-guide/deployment/platform/#create-users","title":"Create Users","text":"<p>Add users in Keycloak, sysadmin users should be in the group <code>admins</code></p>"},{"location":"maintainer-guide/deployment/platform/#change-settings","title":"Change Settings","text":"<p>From sysadmin settings, change the logo with <code>../ckanext-alternative_theme/ckanext/alternative_theme/public/images/fulllogo_transparent.png</code> and update the rest of the options as you wish</p>"},{"location":"maintainer-guide/deployment/platform/#restore-ckan-postgresql-backup","title":"Restore CKAN PostgreSQL Backup","text":"<ol> <li> <p>Copy the <code>.dump</code> file to the DB pod <pre><code>kubectl cp postgres.dump postgres-0:/tmp/backup.dump\n</code></pre></p> </li> <li> <p>Get PostgreSQL password <pre><code>kubectl get secret postgrescredentials -o jsonpath='{.data.postgresql-password}' | base64 --decode\n</code></pre></p> </li> <li> <p>Enter DB pod <pre><code>kubectl exec -it postgres-0 /bin/bash\n</code></pre></p> </li> <li> <p>Set environment variables (replace <code>pass</code> with the PostgreSQL password) <pre><code>export PGDATABASE=ckan_default\nexport PGUSER=postgres\nexport PGPASSWORD=pass\n</code></pre></p> </li> <li> <p>Restore the DB from the backup file <pre><code>pg_restore -d $PGDATABASE /tmp/backup.dump --clean --if-exists\n</code></pre></p> </li> <li> <p>Exit the DB pod with <code>exit</code> and enter the CKAN pod (replace <code>ckan-pod</code> with the actual pod name) <pre><code>kubectl exec -it ckan-pod /bin/bash\n</code></pre></p> </li> <li> <p>Rebuild the search index for datasets to be listed correctly <pre><code>ckan -c production.ini search-index rebuild\n</code></pre></p> </li> </ol>"},{"location":"maintainer-guide/deployment/platform/#install-jupyterhub","title":"Install Jupyterhub","text":""},{"location":"maintainer-guide/deployment/platform/#create-certificate_1","title":"Create Certificate","text":"<ol> <li>Update <code>dnsNames</code> and <code>issuerRef</code> params in <code>jupyterhub/manifests/certificate.yaml</code></li> <li>Create the certificate resource <pre><code>kubectl apply -f ./jupyterhub/manifests/certificate.yaml\n</code></pre></li> </ol>"},{"location":"maintainer-guide/deployment/platform/#create-ingress_1","title":"Create Ingress","text":"<ol> <li>Update <code>tls</code> and <code>rules</code> params in <code>jupyterhub/manifests/ingress.yaml</code></li> <li>Create the ingress resource <pre><code>kubectl apply -f ./jupyterhub/manifests/ingress.yaml\n</code></pre></li> </ol>"},{"location":"maintainer-guide/deployment/platform/#create-shared-jupyter-volume","title":"Create Shared Jupyter Volume","text":"<ol> <li>Create NFS required pvc resource <pre><code>kubectl apply -f ./jupyterhub/manifests/nfs/pvc.yaml\n</code></pre></li> <li>Create NFS resources <pre><code>kubectl apply -f ./jupyterhub/manifests/nfs/deployment.yaml\nkubectl apply -f ./jupyterhub/manifests/nfs/service.yaml\n</code></pre></li> <li>Create Persistent volume required for shared PVC <pre><code>kubectl apply -f ./jupyterhub/manifests/nfs/pv.yaml\n</code></pre></li> <li>Create shared PVC <pre><code>kubectl apply -f ./jupyterhub/manifests/pvc.yaml\n</code></pre></li> </ol>"},{"location":"maintainer-guide/deployment/platform/#build-custom-image","title":"Build Custom Image","text":"<ol> <li>Build a new docker image <pre><code>DOCKER_BUILDKIT=1 docker build -f ./jupyterhub/singleuser/Dockerfile ./jupyterhub/singleuser/ -t alternative.cr.de-fra.ionos.com/alternative-singleuser:v0.0.7\n</code></pre></li> <li>Push the new image <pre><code>docker push alternative.cr.de-fra.ionos.com/alternative-singleuser:v0.0.7\n</code></pre></li> </ol>"},{"location":"maintainer-guide/deployment/platform/#install-helm-chart","title":"Install Helm Chart","text":"<ol> <li>Update the parameters in <code>jupyterhub/config.yaml</code></li> <li>Install helm chart <pre><code>helm install -f ./jupyterhub/config.yaml alternative-jupyterhub ./jupyterhub/chart/ --version=2.0.0\n</code></pre></li> </ol>"},{"location":"maintainer-guide/deployment/platform/#backup-jobs","title":"Backup Jobs","text":"<ol> <li>Update the configurations in <code>deployment/manifests/backup_job.yaml</code> and <code>deployment/manifests/backup_credentials.yaml</code></li> <li>Apply the files <pre><code>kubectl apply -f ./deployment/manifests/backup_credentials.yaml\nkubectl apply -f ./deployment/manifests/backup_job.yaml\n</code></pre></li> </ol>"},{"location":"maintainer-guide/features/data-sharing/","title":"Data sharing","text":""},{"location":"maintainer-guide/features/data-sharing/#overview","title":"Overview","text":"<p>The ALTERNATIVE platform facilitates secure data sharing and exchange between authorized users and groups through its comprehensive access control capabilities. All datasets and resources uploaded to the platform can be assigned granular permissions, specifying which users or organizations are allowed to view, access for analysis, edit metadata, or delete the assets entirely.</p> <p>Flexible organization hierarchies allow reflective sharing permissions, such as a folder visible to members of a specific department, or an analytics project workspace shared between different users collaborating tightly. Access requests can also be configured for privileged approvals before enabling wider dataset visibility.</p> <p>Published open data is made publicly findable and accessible to all CKAN visitors. Private data restricted to members of the same group or organization can be easily shared for collaboration scenarios with audit logs tracking all access. Restricted datasets also benefit from CKAN features like ratings/comments, metadata enhancements, and discussions while remaining protected.</p>"},{"location":"maintainer-guide/features/data-sharing/#metadata","title":"Metadata","text":"<p>Metadata helps organize, find, and understand information better, acting as a helpful guide that tells you what you need to know about the stuff you're looking at or working with. The Platform provides a powerful metadata functionality, including default fields, custom fields and metadata extension for scientific experiments.</p>"},{"location":"maintainer-guide/features/data-sharing/#default-metadata-fields","title":"Default Metadata Fields","text":"name description name unique across the platform, should be brief but specific URL automatically filled based on title, but can be edited Description longer description of the dataset, including information such as what people need to know when using the data Tags tags that help people find the data linked with other related data License it is important to include a license, so that people know how they can use the data Organization organization, owner of the dataset Visibility public or private visibility of the dataset Source where the data is from Version version of the data Author name of the person or organization responsible for producing the data Author email an email address of the author, to which queries about the data should be sent Maintainer name of the person or organization responsible for maintaining the data Maintainer email details for a second person responsible for the data"},{"location":"maintainer-guide/features/data-sharing/#advanced-metadata-for-experiments","title":"Advanced Metadata for Experiments","text":"<p>Because the project and datasets specifics related to experiment data, predefined metadata fields were implemented. When creating or editing a dataset a checkbox located above the custom metadata fields is available to show these extra fields.</p> name description Culture medium select culture medium between 2 options (EBM2 or Maintenance) Number of replicates set number of replicates Number of cells/well set number of cells/well Ratio iPSC-CMs/HCAECs set ratio iPSC-CMs/HCAECs Date of experiment the date of the experiment Toxin select toxin between 4 options (Ami, Blank, Dox, TBBPA) Age type select age type between 2 options (Old or Young) Dimension select dimension between 2 options (2D or 3D) Category select category between 6 options (Chip sensors, In vitro assays, Metabolomics, Proteomics, Toxin targeted metabolomics, Transcriptomics) Content select content between 3 options (Cells, Culture media, Not applicable) Model select model between 2 options (Dynamic or Static)"},{"location":"maintainer-guide/features/data-sharing/#custom-metadata-fields","title":"Custom Metadata Fields","text":"<p>Storing additional metadata for a dataset beyond the default metadata is a common use case. The ALTERNATIVE platform provides a simple way to do this by allowing you to store arbitrary key/value pairs against a dataset when creating or updating the dataset. These appear under the \u201cAdvanced metadata for experiments\u201d section on the web interface and in the \u2018extras\u2019 field of the JSON when accessed via the API.</p>"},{"location":"maintainer-guide/features/development-environment/","title":"JupyterHub Development Environment","text":""},{"location":"maintainer-guide/features/development-environment/#overview","title":"Overview","text":"<p>JupyterHub provides a multi-user development environment for data scientists and engineers collaborating on analytical workloads. It offers on-demand access to ephemeral notebook servers and IDEs, dynamically configured with language kernels, libraries, attached storage volumes, and access privileges based on user and context.</p>"},{"location":"maintainer-guide/features/development-environment/#key-advantages","title":"Key Advantages","text":""},{"location":"maintainer-guide/features/development-environment/#scalability","title":"Scalability","text":"<ul> <li>Provisioning: JupyterHub handles provisioning secure single-user workspaces on a Kubernetes cluster to serve multiple users simultaneously.</li> <li>Workspace Isolation: Allows for personalized tool and dependency configurations per project needs.</li> </ul>"},{"location":"maintainer-guide/features/development-environment/#rich-interface","title":"Rich Interface","text":"<ul> <li>Language Support: Supports kernels like Python, R, and Scala for code, visualizations, and documentation.</li> <li>Integration: Integrates with version control systems and data platforms.</li> </ul>"},{"location":"maintainer-guide/features/development-environment/#security-and-governance","title":"Security and Governance","text":"<ul> <li>Self-Service Access: Enables self-service access to data science workbenches with security and governance.</li> <li>DevOps Automation: Provides devops service delivery advantages through Kubernetes backend automation.</li> </ul>"},{"location":"maintainer-guide/features/development-environment/#collaboration-and-sharing","title":"Collaboration and Sharing","text":"<ul> <li>Notebook Sharing: Notebooks can be easily shared between users and published to broader teams.</li> <li>Standardized Workflows: Facilitates the creation of standardized workflows or ML models.</li> </ul>"},{"location":"maintainer-guide/features/development-environment/#community-and-extensions","title":"Community and Extensions","text":"<ul> <li>Community-Sourced Extensions: The workspace environment evolves through community-sourced JupyterHub extensions.</li> </ul>"},{"location":"maintainer-guide/features/elasticity/","title":"Elasticity","text":""},{"location":"maintainer-guide/features/elasticity/#overview","title":"Overview","text":"<p>Elasticity is a cornerstone feature of the ALTERNATIVE platform's cloud infrastructure. It refers to the ability of the system to dynamically adjust its resource allocation in response to changing demands, ensuring optimal performance and cost-efficiency.</p>"},{"location":"maintainer-guide/features/elasticity/#key-aspects-of-elasticity","title":"Key Aspects of Elasticity","text":""},{"location":"maintainer-guide/features/elasticity/#1-on-demand-provisioning","title":"1. On-Demand Provisioning","text":"<ul> <li>Dynamic Resource Allocation: Services can be provisioned instantly as needed.</li> </ul>"},{"location":"maintainer-guide/features/elasticity/#2-scalability","title":"2. Scalability","text":"<ul> <li>Vertical Scaling: Ability to increase or decrease the power of individual resources.</li> <li>Horizontal Scaling: Capability to add or remove instances of resources.</li> </ul>"},{"location":"maintainer-guide/features/elasticity/#3-adaptability","title":"3. Adaptability","text":"<ul> <li>Workload Fluctuations: The infrastructure adapts to varying computational demands.</li> <li>Project Evolution: Easily accommodates changes in project requirements over time.</li> </ul>"},{"location":"maintainer-guide/features/elasticity/#benefits-of-elasticity-in-alternative","title":"Benefits of Elasticity in ALTERNATIVE","text":"<ol> <li>Cost Optimization: Pay only for the resources you use, avoiding over-provisioning.</li> <li>Performance Enhancement: Maintain responsiveness during peak demand periods.</li> <li>Resource Efficiency: Automatically scale down during low-usage periods.</li> <li>Future-Proofing: Easily adapt to new technologies and methodologies as the project evolves.</li> </ol>"},{"location":"maintainer-guide/features/elasticity/#implementation-in-alternative","title":"Implementation in ALTERNATIVE","text":"<ul> <li>Cloud Provider Services: Utilizes native cloud services for elastic compute and storage resources.</li> </ul>"},{"location":"maintainer-guide/features/elasticity/#use-cases","title":"Use Cases","text":"<ol> <li>Data Processing Pipelines: Automatically scale computational resources for large-scale data analysis tasks.</li> <li>User Traffic Spikes: Seamlessly handle increased user activity during collaborative sessions or data uploads.</li> </ol>"},{"location":"maintainer-guide/features/extensions/","title":"ALTERNATIVE Platform Extensions","text":""},{"location":"maintainer-guide/features/extensions/#overview","title":"Overview","text":"<p>The CKAN extension mechanism is a powerful framework that enhances and customizes the functionality of the CKAN open-source data management system. This extensibility is crucial for the ALTERNATIVE cloud data platform, allowing us to tailor CKAN to our specific needs and requirements.</p>"},{"location":"maintainer-guide/features/extensions/#benefits-of-ckan-extensions","title":"Benefits of CKAN Extensions","text":"<ul> <li>Customization: Adapt CKAN to specific project needs</li> <li>Enhanced Functionality: Add new features beyond CKAN's core capabilities</li> <li>Improved User Experience: Customize the interface and workflows</li> <li>Integration: Connect CKAN with other systems and services</li> <li>Scalability: Extend the platform's capabilities as project needs evolve</li> </ul>"},{"location":"maintainer-guide/features/extensions/#alternative-specific-extensions","title":"ALTERNATIVE-Specific Extensions","text":"<p>The following extensions have been developed and implemented specifically for the ALTERNATIVE platform:</p>"},{"location":"maintainer-guide/features/extensions/#1-ckanext-alternative_theme","title":"1. ckanext-alternative_theme","text":"<p>Purpose: Customizes the visual appearance of the platform</p> <p>Key Features:</p> <ul> <li>Custom color scheme aligned with ALTERNATIVE branding</li> <li>Modified layout for improved user navigation</li> <li>Responsive design for various device types</li> </ul>"},{"location":"maintainer-guide/features/extensions/#2-ckanext-cloudstorage","title":"2. ckanext-cloudstorage","text":"<p>Purpose: Implements support for S3 Cloud Storage</p> <p>Key Features:</p> <ul> <li>Integration with S3-compatible storage services</li> <li>Efficient handling of large datasets</li> <li>Customized from the original ckanext-cloudstorage extension</li> <li>Uses libcloud for improved cloud storage interactions</li> </ul>"},{"location":"maintainer-guide/features/extensions/#3-ckanext-keycloak_auth","title":"3. ckanext-keycloak_auth","text":"<p>Purpose: Enables Keycloak authentication and user management</p> <p>Key Features:</p> <ul> <li>Single Sign-On (SSO) capabilities</li> <li>Integration with Keycloak Identity and Access Management</li> <li>Enhanced security for user authentication</li> <li>Role-based access control</li> </ul>"},{"location":"maintainer-guide/features/extensions/#4-ckanext-extrafields","title":"4. ckanext-extrafields","text":"<p>Purpose: Adds additional fields to dataset metadata</p> <p>Key Features:</p> <ul> <li>Custom fields for experiment information</li> <li>Size metadata for improved dataset management</li> <li>Extensible structure for future metadata requirements</li> </ul>"},{"location":"maintainer-guide/features/extensions/#5-ckanext-keycloak_access_token","title":"5. ckanext-keycloak_access_token","text":"<p>Purpose: Enables management of AI/ML API tokens in Keycloak</p> <p>Key Features:</p> <ul> <li>Secure token generation and management</li> <li>Integration with AI/ML services</li> <li>User-specific API access control</li> <li>Token lifecycle management</li> </ul>"},{"location":"maintainer-guide/features/extensions/#implementing-and-managing-extensions","title":"Implementing and Managing Extensions","text":"<ol> <li>Installation: Extensions are installed in the CKAN environment using pip or by cloning the extension's repository.</li> <li>Configuration: Each extension is configured in the CKAN configuration file (<code>production.ini</code> or <code>development.ini</code>).</li> <li>Activation: Extensions are activated by adding them to the <code>ckan.plugins</code> setting in the configuration file.</li> <li>Maintenance: Regular updates and testing ensure compatibility with the core CKAN system and other extensions.</li> </ol> <p>By leveraging these extensions, the ALTERNATIVE platform provides a tailored, robust, and feature-rich environment for managing and sharing scientific data and resources.</p>"},{"location":"maintainer-guide/features/multi-user-and-collaboration/","title":"Multi-user &amp; Collaboration Support","text":""},{"location":"maintainer-guide/features/multi-user-and-collaboration/#overview","title":"Overview","text":"<p>The ALTERNATIVE platform is designed with robust multi-user support and collaboration features, recognizing the diverse needs of project researchers, partner company scientists, and designated regulators. These features are integral to the platform's architecture, ensuring secure and efficient teamwork across various user groups.</p>"},{"location":"maintainer-guide/features/multi-user-and-collaboration/#key-collaboration-features","title":"Key Collaboration Features","text":"<ol> <li> <p>Secure Data Sharing: The platform enables controlled access to datasets and analytical workflows, promoting transparency while maintaining data security.</p> </li> <li> <p>Multi-tenant Jupyter Notebooks: Users can work in isolated environments with selective content visibility, allowing for personalized workspaces that still support collaboration.</p> </li> <li> <p>CKAN Customization: Through CKAN extension points, the platform offers tailored experiences for different user groups, enhancing usability and efficiency.</p> </li> <li> <p>Single Sign-On (SSO): Seamless authentication across Data Management part and JupyterHub components, powered by Keycloak, ensures a smooth user experience.</p> </li> <li> <p>Flexible Organization Hierarchies: Reflective sharing permissions allow for nuanced access control, such as department-specific visibility or project-based collaboration spaces.</p> </li> </ol>"},{"location":"maintainer-guide/features/multi-user-and-collaboration/#dataset-collaboration","title":"Dataset Collaboration","text":""},{"location":"maintainer-guide/features/multi-user-and-collaboration/#dataset-collaborators","title":"Dataset Collaborators","text":"<p>The Collaborators tab provides granular control over dataset access:</p> <ul> <li>Viewing Collaborators: Lists all users with special permissions for a dataset, including those outside the dataset's owning organization.</li> <li>Managing Roles: Allows for adding, removing, or modifying collaborator roles directly from the interface.</li> </ul>"},{"location":"maintainer-guide/features/multi-user-and-collaboration/#collaborator-roles-and-permissions","title":"Collaborator Roles and Permissions","text":"<ol> <li> <p>Member</p> <ul> <li>Can access private datasets</li> <li>Cannot edit dataset content</li> </ul> </li> <li> <p>Editor</p> <ul> <li>Full access to private datasets</li> <li>Can edit dataset content and resources</li> </ul> </li> <li> <p>Admin</p> <ul> <li>All Editor permissions</li> <li>Can manage collaborator list (add/remove users, change roles)</li> </ul> </li> </ol>"},{"location":"maintainer-guide/features/multi-user-and-collaboration/#collaboration-workflow","title":"Collaboration Workflow","text":"<ol> <li>Data Publishing: Organizations can publish datasets with specified visibility (public or private).</li> <li>Access Control: Admins can set granular permissions for datasets and resources.</li> <li>Collaboration Spaces: Users can create shared workspaces for specific projects or teams.</li> <li>Version Control: Built-in support for tracking changes and maintaining data integrity.</li> <li>Feedback Mechanisms: Users can rate, comment on, and discuss datasets within the platform.</li> </ol>"},{"location":"maintainer-guide/features/multi-user-and-collaboration/#best-practices-for-collaboration","title":"Best Practices for Collaboration","text":"<ul> <li>Regularly review and update collaborator permissions to maintain security.</li> <li>Utilize tagging and metadata to improve discoverability of shared resources.</li> <li>Leverage Jupyter notebooks for interactive data exploration and sharing of analytical workflows.</li> <li>Encourage the use of version control features to track changes and facilitate collaboration on datasets and code.</li> </ul>"},{"location":"maintainer-guide/features/overview/","title":"Platform Features","text":""},{"location":"maintainer-guide/features/overview/#overview","title":"Overview","text":"<p>The ALTERNATIVE platform offers a comprehensive set of features designed to support collaborative research, data management, and analysis in the field of environmental toxicology. These features are built upon a robust cloud infrastructure, leveraging technologies such as Kubernetes, CKAN, and JupyterHub to provide a scalable and user-friendly environment for scientists and researchers.</p>"},{"location":"maintainer-guide/features/overview/#data-sharing","title":"Data Sharing","text":"<p>The platform's data sharing capabilities are primarily powered by CKAN (Comprehensive Knowledge Archive Network), a web-based catalog platform for managing large datasets collaboratively. Key features include:</p> <ul> <li>Organization-based Access Control: Each consortium partner is represented as an organization, with customizable workflows and authorization mechanisms.</li> <li>Flexible User Roles: Users can be assigned roles such as Member, Editor, or Admin, each with varying levels of access and control.</li> <li>Rich Metadata Support: Datasets include default metadata fields, custom fields, and specialized metadata for scientific experiments.</li> <li>Resource Management: Multiple resources can be associated with each dataset, supporting various file formats and external links.</li> <li>Data Visibility Control: Datasets can be set as public or private, allowing for both open data sharing and restricted access scenarios.</li> <li>Dataset Collaboration: Users can be added as collaborators on specific datasets, with granular permission settings.</li> </ul>"},{"location":"maintainer-guide/features/overview/#development-environment","title":"Development Environment","text":"<p>JupyterHub is integrated into the platform to provide a powerful development environment for researchers and data scientists. Features include:</p> <ul> <li>On-demand JupyterLab Servers: Users get personalized workspaces with a rich set of tools for data analysis and visualization.</li> <li>Multiple Programming Environments: Support for Python, R, and other languages, with pre-configured environments and libraries.</li> <li>Integration with Platform Data: Easy access to datasets stored in the Platform through custom Python libraries.</li> <li>Resource Optimization: Automatic shutdown of inactive servers to manage platform resources efficiently.</li> <li>Customizable Workspace: Users can configure their JupyterLab interface with various plugins and extensions.</li> </ul>"},{"location":"maintainer-guide/features/overview/#multi-user-and-collaboration","title":"Multi-user and Collaboration","text":"<p>The platform is designed to facilitate collaboration among diverse teams of researchers, scientists, and regulators. Collaborative features include:</p> <ul> <li>Shared Datasets: Users can share datasets within organizations or with specific collaborators.</li> <li>Collaborative Notebooks: JupyterHub supports multi-user access to notebooks, enabling real-time collaboration.</li> <li>Version Control Integration: Built-in support for Git, allowing users to manage and share code effectively.</li> <li>Customizable User Experiences: Platform extensions allow for tailored interfaces for different user groups.</li> </ul>"},{"location":"maintainer-guide/features/overview/#single-sign-on","title":"Single Sign-On","text":"<p>To provide a seamless user experience across different components of the platform, Single Sign-On (SSO) has been implemented:</p> <ul> <li>Keycloak Integration: Uses Keycloak for centralized identity and access management.</li> <li>Unified Authentication: Users can access both Data Platform and JupyterHub with a single set of credentials.</li> <li>Secure Access Control: Supports fine-grained access control and advanced security features like Two-Factor Authentication (2FA).</li> </ul>"},{"location":"maintainer-guide/features/overview/#elasticity","title":"Elasticity","text":"<p>The platform leverages cloud infrastructure and Kubernetes to provide elastic scaling capabilities:</p> <ul> <li>Dynamic Resource Allocation: Kubernetes enables automatic scaling of resources based on demand.</li> <li>Efficient Resource Utilization: Services can scale up or down to optimize performance and cost.</li> <li>Storage Flexibility: Utilizes various storage solutions, including block storage and S3, to accommodate different data storage needs.</li> <li>On-demand Compute: JupyterHub spawns user environments on-demand, efficiently managing computational resources.</li> </ul> <p>These features collectively create a powerful, flexible, and secure environment for collaborative research and data analysis in the ALTERNATIVE project, supporting the complex requirements of environmental toxicology studies.</p>"},{"location":"maintainer-guide/features/sso/","title":"Single Sign-On","text":""},{"location":"maintainer-guide/features/sso/#single-sign-on-sso","title":"Single Sign-On (SSO)","text":"<p>The ALTERNATIVE platform implements a robust Single Sign-On (SSO) solution, which is a key integration between the Platform web application and the JupyterHub development environment. This feature enhances user experience and security across the platform.</p>"},{"location":"maintainer-guide/features/sso/#implementation-with-keycloak","title":"Implementation with Keycloak","text":"<p>The SSO functionality is powered by Keycloak, an open-source Identity and Access Management solution. Keycloak serves as the central authentication and authorization server for the ALTERNATIVE platform.</p> <p>Key aspects of the Keycloak-based SSO implementation include:</p> <ol> <li> <p>Centralized Authentication: Users authenticate once through Keycloak and gain access to multiple applications within the ALTERNATIVE ecosystem.</p> </li> <li> <p>Seamless User Experience: From the web UI, users can launch Jupyter workspaces without the need for reauthentication, providing a smooth and efficient workflow.</p> </li> <li> <p>Enhanced Security: By centralizing authentication, Keycloak helps maintain consistent security policies across all integrated applications.</p> </li> <li> <p>Protocol Support: Keycloak implements industry-standard protocols such as OpenID Connect (OIDC) and OAuth 2.0, ensuring compatibility and security.</p> </li> </ol>"},{"location":"maintainer-guide/features/sso/#benefits-of-sso-in-alternative","title":"Benefits of SSO in ALTERNATIVE","text":"<p>The implementation of SSO brings several advantages to the ALTERNATIVE platform:</p> <ul> <li>Improved User Productivity: Users spend less time managing multiple credentials and logging in to different systems.</li> <li>Reduced Password Fatigue: With only one set of credentials to remember, users are less likely to resort to insecure password practices.</li> <li>Streamlined Access Management: Administrators can manage user access to multiple applications from a single point, simplifying user provisioning and deprovisioning.</li> <li>Enhanced Security Monitoring: Centralized authentication allows for better tracking and auditing of user access across the platform.</li> </ul>"},{"location":"maintainer-guide/features/sso/#sso-workflow","title":"SSO Workflow","text":"<ol> <li>User logs in to the ALTERNATIVE web application using their Keycloak credentials.</li> <li>Upon successful authentication, Keycloak issues a secure token.</li> <li>When the user accesses JupyterHub or other integrated services, the token is used to authenticate the user automatically.</li> <li>The user can seamlessly navigate between Data Management part, JupyterHub, and other platform components without additional login prompts.</li> </ol>"},{"location":"maintainer-guide/system-services/overview/","title":"System Services","text":""},{"location":"maintainer-guide/system-services/overview/#overview","title":"Overview","text":"<p>The system services provided by the ALTERNATIVE platform facilitate direct interaction with the platform's various features and tools.</p>"},{"location":"maintainer-guide/system-services/overview/#ckan","title":"CKAN","text":"<p>The ALTERNATIVE platform's data sharing component utilizes CKAN, a web-based catalog platform designed for collaborative handling of large datasets. CKAN offers an intuitive web interface for creating, sharing, and managing different types of data with both internal teams and external audiences.</p>"},{"location":"maintainer-guide/system-services/overview/#users-organizations-and-authorization","title":"Users, Organizations, and Authorization","text":"<p>CKAN, central to the ALTERNATIVE project, classifies users into regular users and sysadmin users. Sysadmin users can create organizations within the platform. Initially, an organization contains no datasets and has a single member, typically the creator. Unregistered users can search for and access public data, but registration is required for publishing activities and accessing personalization features. User identity and access management within CKAN is handled by Keycloak.</p> <p>In the ALTERNATIVE platform, each consortium partner is represented as an organization with unique workflows and authorization mechanisms, allowing autonomous management of their publishing processes. Organizations control access to datasets, determining who can view, create, and modify them. Administrative personnel within an organization can add users and assign roles with varying levels of access and control:</p> <ul> <li>Member Role: Can view private datasets owned by the organization.</li> <li>Editor Role: Can edit and publish datasets in addition to viewing them.</li> <li>Admin Role: Can add, remove, and modify the roles of organization members, alongside all privileges of an Editor.</li> </ul> <p>This structured approach ensures a streamlined and secure process for dataset management and publication, allowing each organization to maintain data integrity and confidentiality while facilitating collaboration and data sharing as per project objectives.</p> <p></p>"},{"location":"maintainer-guide/system-services/overview/#datasets-resources-and-groups","title":"Datasets, Resources, and Groups","text":"<p>The ALTERNATIVE platform handles data publication through \"datasets,\" which are collections of metadata describing the data, along with various resources containing the actual data. The platform supports multiple data formats, including CSV, Excel, XML, PDF, image files, and linked data in RDF format. Resources are primarily stored in an S3 bucket within Cloud Storage but can also be external web links.</p> <ul> <li>Dataset Exploration: The platform offers a comprehensive interface for exploring datasets. Users can view a complete list of available datasets through the datasets menu or an organization's page. Upon selecting a dataset, its page is displayed with three main tabs:<ul> <li>Dataset Tab: Presents detailed information about the dataset, including a list of its resources.</li> <li></li> <li>Groups Tab: Manages which groups of users can access the dataset.</li> <li>Activity Stream Tab: Chronicles the historical changes made to the dataset, providing a transparent audit trail.</li> </ul> </li> <li>Dataset Creation: Creating a new dataset is intuitive. Users can initiate dataset creation from the 'Datasets' page by clicking 'Add Dataset' or from the 'Organisations' page by selecting the appropriate owning organization.</li> <li></li> <li>Dataset Management: Management privileges are based on user roles and associations. Users can manage any dataset they have created, any dataset owned by their organization, or any dataset where they are designated as a collaborator with at least an Editor role.</li> <li>Resource Management within a Dataset: Managing a dataset's resources is facilitated through the 'Resources' tab. Users can add new resources using the 'Add new resource' button, edit existing resources, and finalize changes using the 'Update Resources' button.</li> <li>Metadata: Each dataset contains metadata defined during creation or later. Metadata includes information like author, organization, and license, used for querying datasets. CKAN has been extended with custom metadata fields under \"Advanced metadata for experiments,\" tailored for datasets in ALTERNATIVE\u2019s scientific base.</li> </ul> <p></p>"},{"location":"maintainer-guide/system-services/overview/#jupyterhub","title":"JupyterHub","text":"<p>JupyterHub is integrated into the ALTERNATIVE platform, offering researchers and scientists an environment for working with datasets and analytical workflows.</p> <p>Authenticated users access JupyterHub via a single sign-on URL from the web portal, which spawns a dedicated JupyterHub server dynamically for them. This personalized workspace includes tools such as Jupyter notebooks, text editors, terminals, code consoles, file browsers, data visualization, and version control integration.</p> <p>Jupyter Notebooks are versatile documents that integrate executable code, mathematical equations, graphics, and interactive visualizations. They facilitate exploratory analysis over datasets and rapid development of reusable scripts, machine learning pipelines, models, and applications in languages like Python and R.</p> <p>To optimize resource usage, ephemeral notebook servers are automatically shut down after periods of inactivity, preserving workflow outputs and snapshots for future access. This balances responsive on-demand environments with efficient platform cost management at scale.</p> <p>JupyterHub offers extensive customization options after server provisioning: - File Browser Pane: Access control for data and workspace assets. - Tabbed Editor: Concurrently work across documents like notebooks, code files, and markdown reports. - Terminals: Manage dependencies or launch Docker environments. - Sidebar Tools: Version control plugins, debuggers, data visualization toolkits, and an extensions catalog. - Global User Preferences: Customize UI behavior, notebook runtimes, and resource visualization. - Keyboard Shortcuts: Expedite navigation and frequent tasks. - Hub Control Panel: Reconnect to central management consoles. - GitHub Integration Plugin: Available in the sidebar.</p> <p>An example of a standard data science-focused Jupyter notebook illustrates these integrated capabilities:</p> <p></p> <p>Each JupyterHub server includes two environments at startup: a default Python environment and an additional Conda environment. Users can choose the environment for their notebooks or consoles via the Launcher tab or Terminal:</p> <ul> <li>Python Virtual Environments: Independent sets of Python packages, isolated from the base environment.</li> <li>Conda Environments: Independent sets of Python or Conda packages.</li> <li>Python Library: The alternative-lib helps find datasets and download resources using ckanapi.</li> <li>R Environment: For statistical computing and graphics, accessible from the Terminal.</li> <li>Bioconductor Integration: A free toolkit for analyzing genetic data, integrated through R programming and Docker images.</li> </ul>"},{"location":"maintainer-guide/system-services/overview/#aiml-api","title":"AI/ML API","text":"<p>The AI/ML API is a crucial component of the ALTERNATIVE platform, offering users access to a variety of machine learning models and algorithms. These models support data analysis, predictive modeling, and decision-making. Built on a scalable and efficient infrastructure, the AI/ML API allows users to run complex machine learning tasks with ease through a set of RESTful endpoints, which can be integrated into existing applications or used directly by users.</p>"},{"location":"maintainer-guide/system-services/ai-ml-api/","title":"AI/ML API Server","text":"<p>This server a part of the ALTERNATIVE project, providing a robust and secure interface for accessing a wide range of machine learning models. Our goal is to simplify the integration of ML models into various applications and workflows, ensuring seamless access and efficient operation.</p>"},{"location":"maintainer-guide/system-services/ai-ml-api/#overview","title":"Overview","text":"<p>The AI/ML API Server is engineered to support high-demand scenarios, offering a unified interface for machine learning models developed by consortium partners. It ensures seamless integration, secure access, and efficient operation, catering to a variety of use cases from predictive analytics to real-time data processing.</p>"},{"location":"maintainer-guide/system-services/ai-ml-api/#key-objectives","title":"Key Objectives","text":"<ul> <li>Seamless Integration: Simplify the incorporation of ML models into existing applications and workflows.</li> <li>Secure API Access: Implement state-of-the-art security measures for data protection and access control.</li> <li>Scalable Architecture: Dynamically adjust resources to handle varying loads, ensuring consistent performance.</li> <li>High Availability: Design for fault tolerance and resilience to minimize downtime.</li> <li>Comprehensive Documentation: Provide detailed guides and examples to facilitate easy adoption.</li> <li>User-Friendly Interfaces: Offer intuitive tools for managing API tokens and accessing model functionalities.</li> </ul>"},{"location":"maintainer-guide/system-services/ai-ml-api/#features","title":"Features","text":"<ul> <li>Diverse Model Support: Access a wide range of ML models for different domains and applications.</li> <li>Token-Based Authentication: Secure API endpoints with robust token authentication mechanisms.</li> <li>Scalable Deployment: Leverage Docker and Kubernetes for scalable and manageable deployments.</li> <li>Performance Monitoring: Integrated tools for tracking API performance and usage statistics.</li> <li>Interactive Documentation: Explore API functionalities with interactive Swagger documentation.</li> </ul>"},{"location":"maintainer-guide/system-services/ai-ml-api/data-flow/","title":"Data Flow","text":"<p>The data flow process involves several key steps, outlined below:</p> <ol> <li> <p>Client Request</p> <ul> <li>The client application sends an API request to the system. This request includes an authentication token and the necessary data or parameters for the API endpoint.</li> </ul> </li> <li> <p>Istio Service Mesh</p> <ul> <li>The request first passes through the Istio Service Mesh.</li> </ul> </li> <li> <p>Envoy Filter Interception</p> <ul> <li>The custom Envoy Filter, written in Go, intercepts the request and begins processing.</li> </ul> </li> <li> <p>Public Key Retrieval</p> <ul> <li>The Envoy Filter fetches and caches the public key from Keycloak, which will be used to verify the authentication token.</li> </ul> </li> <li> <p>Token Verification</p> <ul> <li>The Envoy Filter verifies the token's signature using the fetched public key. It also checks the token's validity, expiration date, and any other claims.</li> </ul> </li> <li> <p>Role and Permission Check</p> <ul> <li>After verifying the token, the Envoy Filter checks the user's roles and permissions encoded within the token.</li> </ul> </li> <li> <p>Revoked Token Check</p> <ul> <li>The Envoy Filter fetches and caches revoked tokens from the database, then checks if the token is revoked. If revoked, it responds with an unauthorized message to the client.</li> </ul> </li> <li> <p>Forward Valid Request</p> <ul> <li>If the token is valid and the user has the necessary roles and permissions, the Envoy Filter forwards the request to the AI/ML API Server.</li> </ul> </li> <li> <p>ML Model Interaction</p> <ul> <li>The API Server interacts with the appropriate machine learning (ML) model(s) based on the functionality requested.</li> </ul> </li> <li> <p>ML Model Processing</p> <ul> <li>The ML model(s) process the input data and generate the requested output, such as predictions, classifications, or analyses.</li> </ul> </li> <li> <p>API Response</p> <ul> <li>The API Server packages the output from the ML model(s) into an appropriate response format and sends it back to the client application through the secure communication channel.</li> </ul> </li> </ol>"},{"location":"maintainer-guide/system-services/ai-ml-api/scalability/","title":"Scalability","text":"<p>To accommodate varying loads and optimize resource utilization, our system employs a multi-faceted approach to scalability. This document outlines the key strategies used to ensure that the system can handle growth in demand without compromising on performance.</p>"},{"location":"maintainer-guide/system-services/ai-ml-api/scalability/#horizontal-scaling","title":"Horizontal Scaling","text":"<p>Horizontal scaling, also known as scaling out, involves adding more instances of the API servers to distribute the load evenly. This approach is particularly effective for handling an increase in user requests.</p> <ul> <li> <p>Stateless API Servers: Our API servers are designed to be stateless, which means they do not store any user data between requests. This design choice allows us to add or remove server instances without impacting the system's state or performance.</p> </li> <li> <p>Kubernetes Horizontal Node Scaling: We leverage on the Cloud Provider's Kubernetes environment to automatically scale the number of nodes based on resource utilization. This ensures that the system can handle varying loads efficiently.</p> </li> </ul>"},{"location":"maintainer-guide/system-services/ai-ml-api/scalability/#benefits-of-horizontal-scaling","title":"Benefits of Horizontal Scaling","text":"<ul> <li>Flexibility: Easily adjust capacity by adding or removing instances.</li> <li>Fault Tolerance: Reduced impact of a single instance failure on the overall system.</li> <li>Cost-Effectiveness: Pay only for the resources you need, when you need them.</li> </ul>"},{"location":"maintainer-guide/system-services/ai-ml-api/scalability/#auto-scaling","title":"Auto-Scaling","text":"<p>Auto-scaling encompasses both horizontal scaling strategies and applies them automatically in response to traffic patterns and system load.</p> <ul> <li>Burst Scaling: Our system is capable of burst scaling, which is a form of auto-scaling designed to handle sudden spikes in traffic. This ensures that the system remains responsive during unexpected surges in demand.</li> </ul>"},{"location":"maintainer-guide/system-services/ai-ml-api/scalability/#benefits-of-auto-scaling","title":"Benefits of Auto-Scaling","text":"<ul> <li>Responsiveness: Quickly adapts to changes in load, ensuring consistent performance.</li> <li>Cost Efficiency: Resources are scaled up only when needed, reducing unnecessary expenditure.</li> </ul>"},{"location":"maintainer-guide/system-services/ai-ml-api/system-architecture/","title":"System Architecture","text":"<p>The AI/ML API Server is a system designed to provide a unified interface for accessing and utilizing various machine learning (ML) models developed by the consortium partners of the ALTERNATIVE project. The core component of the system is the API Server, which is built using Python, Flask, and Gunicorn.</p>"},{"location":"maintainer-guide/system-services/ai-ml-api/system-architecture/#overview","title":"Overview","text":"<p>When a client application sends a request to the API Server, the request first goes through the Istio Service Mesh, which is responsible for secure communication, monitoring, and traffic management. The request is then intercepted by a custom Envoy Filter, implemented in Go, which handles token-based authentication and authorization by integrating with Keycloak, an identity and access management system.</p> <p>Keycloak manages user identities and access control policies, ensuring that only authorized users can access the API and its resources. The custom Envoy Filter validates the user's token and checks their permissions before allowing the request to proceed to the API Server.</p> <p>If the user is authenticated and authorized, the request is forwarded to the API Server, which interacts with the appropriate ML model(s) to perform the requested task, such as predictive analytics. The ML models are developed using frameworks like TensorFlow, PyTorch, R, and scikit-learn, and are provided by the ALTERNATIVE project participants.</p> <p>The API Server processes the request, obtains the necessary results from the ML model(s), and generates a response, which is then sent back to the client application through the same secure communication channel.</p> <p>The system includes a data access management component that handles API token lifecycle operations. Users can create, renew, and revoke their API tokens through a user-friendly interface. Revoked tokens are recorded in a PostgreSQL database to ensure they cannot be reused for unauthorized access.</p> <p>The entire system is designed to be secure, scalable, and efficient. Security measures include HTTPS encryption, data encryption at rest and in transit, and role-based access control (RBAC) mechanisms. Scalability is achieved through the use of Docker for containerization and Kubernetes for container orchestration, allowing the system to scale up or down as needed to handle increased demand.</p> <p>Overall, the AI/ML API Server provides a centralized and secure way for client applications to access and utilize a variety of machine learning models, while ensuring proper authentication, authorization, and scalability.</p>"},{"location":"maintainer-guide/system-services/ai-ml-api/system-architecture/#components","title":"Components","text":""},{"location":"maintainer-guide/system-services/ai-ml-api/system-architecture/#aiml-api-server","title":"AI/ML API Server","text":"<ul> <li>Description: The AI/ML API server is the core component that exposes the REST API. It handles requests from clients, interfaces with machine learning models, and returns processed results. This server ensures efficient communication between the client applications and the underlying ML models.</li> <li>Responsibilities: <ul> <li>Handling incoming requests from various clients.</li> <li>Interacting with machine learning models to process data.</li> <li>Generating and returning responses based on model outputs.</li> </ul> </li> <li>Technologies: <ul> <li>Python: For implementing the server logic and interfacing with ML models.</li> <li>Flask: As the web framework for creating the RESTful API.</li> <li>Gunicorn: As the WSGI HTTP server for handling concurrent requests and ensuring high performance.</li> </ul> </li> </ul>"},{"location":"maintainer-guide/system-services/ai-ml-api/system-architecture/#istio-service-mesh","title":"Istio Service Mesh","text":"<ul> <li>Description: The Istio Service Mesh is a robust infrastructure layer that provides a uniform approach to securing, connecting, and monitoring microservices. It significantly enhances the security, reliability, and observability of microservices by offering advanced features like mutual TLS authentication, traffic control, and telemetry collection.</li> <li>Responsibilities:<ul> <li>Security: Implementing strong security policies, including mutual TLS and fine-grained access controls.</li> <li>Monitoring: Collecting and analyzing telemetry data to provide deep insights into service performance and health.</li> <li>Traffic Management: Managing traffic flow between services, including load balancing, traffic splitting, and fault injection to ensure smooth and reliable service interactions.</li> </ul> </li> <li>Technologies:<ul> <li>Istio: For service mesh management, providing a comprehensive suite of features to control and observe service interactions.</li> <li>Envoy: As the high-performance proxy that intercepts and routes all traffic within the service mesh, enabling sophisticated traffic management and security enforcement.</li> </ul> </li> </ul>"},{"location":"maintainer-guide/system-services/ai-ml-api/system-architecture/#custom-envoy-filter","title":"Custom Envoy Filter","text":"<ul> <li>Description: The custom Envoy filter, implemented in Go, provides robust token-based authentication and authorization. This filter seamlessly integrates with Keycloak to manage identity and access control, ensuring secure and authorized communication between services.</li> <li>Responsibilities: <ul> <li>Authentication: Verifying the identity of users or services through token validation, ensuring that only authenticated entities can access the services.</li> <li>Authorization: Enforcing access control policies by checking the permissions associated with each token, thus ensuring that only authorized actions are allowed.</li> </ul> </li> <li>Technologies: <ul> <li>Go: For developing the filter, taking advantage of its concurrency features and performance.</li> <li>Envoy: As the service proxy, which the filter extends to handle custom authentication and authorization logic.</li> <li>Keycloak: For identity and access management, providing a centralized platform for authentication and authorization services.</li> </ul> </li> </ul>"},{"location":"maintainer-guide/system-services/ai-ml-api/system-architecture/#keycloak-identity-and-access-management","title":"Keycloak Identity and Access Management","text":"<ul> <li>Description: An open-source system for managing authentication, authorization, and user management services.</li> <li>Responsibilities: Identity management, access control.</li> <li>Technologies: Keycloak.</li> </ul>"},{"location":"maintainer-guide/system-services/ai-ml-api/system-architecture/#docker","title":"Docker","text":"<ul> <li>Description: Docker is a leading containerization platform that packages applications and their dependencies into isolated environments called containers. This ensures consistency and reliability across various deployment platforms, from development to production. It encapsulates the AI/ML API server along with its dependencies into a container, ensuring that it can run consistently across different environments. This containerization simplifies the deployment process and enhances the scalability and manageability of the server.</li> <li>Responsibilities: <ul> <li>Containerization: Encapsulating applications and their dependencies into lightweight, portable containers that can run consistently on any infrastructure.</li> <li>Deployment: Simplifying the deployment process by allowing applications to be deployed in any environment without compatibility issues, enhancing scalability and manageability.</li> </ul> </li> </ul>"},{"location":"maintainer-guide/system-services/ai-ml-api/system-architecture/#kubernetes","title":"Kubernetes","text":"<ul> <li>Description: Kubernetes is a powerful container orchestration platform designed to automate the deployment, scaling, and management of containerized applications. It simplifies complex operations, ensuring applications run smoothly and efficiently across diverse environments. It manages the deployment, scaling, and lifecycle of Docker containers that host the AI/ML API server. By automating these processes, Kubernetes ensures the AI/ML API server can handle varying loads, recover from failures, and maintain high availability.</li> <li>Responsibilities: <ul> <li>Orchestration: Managing the deployment, configuration, and life cycle of containers, ensuring they operate seamlessly together.</li> <li>Scaling: Automatically adjusting the number of running containers based on demand, optimizing resource utilization and application performance.</li> <li>Self-Healing: Monitoring container health and restarting failed containers to maintain high availability.</li> <li>Load Balancing: Distributing network traffic evenly across containers to ensure reliable and efficient application performance.</li> </ul> </li> </ul>"},{"location":"maintainer-guide/system-services/ai-ml-api/system-architecture/#aiml-models","title":"AI/ML Models","text":"<ul> <li>Description: The AI/ML models are diverse machine learning models contributed by participants in the ALTERNATIVE project. These models are utilized for a range of tasks, including image recognition, predictive analytics, natural language processing, and more, to provide intelligent and automated solutions.</li> <li>Responsibilities: <ul> <li>Prediction: Generating predictions based on input data, such as identifying objects in images or forecasting future trends.</li> <li>Inference: Performing real-time inference to provide instant insights and responses in various applications.</li> <li>Learning: Continuously learning and improving from new data to enhance accuracy and performance over time.</li> </ul> </li> <li>Technologies: <ul> <li>TensorFlow: A comprehensive open-source platform for building and deploying machine learning models.</li> <li>PyTorch: A flexible and efficient deep learning framework known for its dynamic computational graph and ease of use.</li> <li>scikit-learn: A robust library for classical machine learning algorithms, offering tools for data preprocessing, model training, and evaluation.</li> </ul> </li> </ul>"},{"location":"maintainer-guide/system-services/ai-ml-api/system-architecture/#postgresql-database","title":"PostgreSQL Database","text":"<ul> <li>Description: PostgreSQL is a powerful, open-source relational database management system (RDBMS) used for storing and managing a wide range of data, including user information, access tokens, and other critical data. Known for its robustness and flexibility, PostgreSQL ensures data integrity and supports advanced data types and performance optimization.</li> <li>Responsibilities: <ul> <li>Data Storage: Efficiently storing and retrieving user data, access tokens, and other related information.</li> <li>Token Management: Maintaining a record of revoked tokens to ensure secure access control and prevent unauthorized access.</li> <li>Data Integrity: Enforcing constraints and transactions to ensure the accuracy and consistency of stored data.</li> </ul> </li> <li>Technologies: <ul> <li>PostgreSQL: The core technology provides advanced features such as ACID compliance, full-text search, and support for JSON and other complex data types, making it suitable for a variety of applications and data management needs.</li> </ul> </li> </ul>"},{"location":"maintainer-guide/system-services/ai-ml-api/testing/","title":"Testing","text":"<p>We have written comprehensive unit tests for the API server, covering a wide range of scenarios and edge cases. It includes the following:</p> <ul> <li>Test Cases: Detailed test cases for each API endpoint.</li> <li>Coverage: Ensuring high test coverage to validate the functionality of the API.</li> <li>Automation: Implementing automated testing to streamline the testing process.</li> <li>Continuous Integration: Integrating testing into the CI/CD pipeline for efficient development.</li> <li>Mocking: Using mocks to simulate external dependencies and ensure isolated testing.</li> <li>Performance Testing: Conducting performance tests to evaluate the API's responsiveness and scalability.</li> </ul>"},{"location":"maintainer-guide/system-services/ai-ml-api/api-usage/authentication/","title":"Authentication","text":"<p>To obtain and use API tokens:</p> <ol> <li>Log in to the ALTERNATIVE platform</li> <li>Navigate to \"Profile\" &gt; \"AI/ML API Tokens\"</li> <li>Specify token name, scopes, and expiration</li> <li>Click \"Create New Token\"</li> <li>Copy the generated token (displayed only once)</li> <li>Use the token in API requests:</li> </ol> <pre><code>curl -X POST \\\n  -H \"Authorization: Bearer &lt;your_token_here&gt;\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"smiles\": \"c1ccccc1O\"}' \\\n  https://ai-ml-api.platform.alternative-project.eu/v1/ai/evaluate\n</code></pre>"},{"location":"maintainer-guide/system-services/ai-ml-api/api-usage/endpoints/","title":"Endpoints","text":"<p>This document provides comprehensive API documentation, ensuring developers have all the necessary information for successful integration.</p>"},{"location":"maintainer-guide/system-services/ai-ml-api/api-usage/endpoints/#openapi-swagger-specification","title":"OpenAPI (Swagger) Specification","text":"<p>Interactive documentation for all endpoints is available through our OpenAPI (Swagger) Specification. This allows for easy testing and exploration of the API capabilities. Access the interactive documentation here. To use it first you need to generate an access token. Then in swagger you need to enter your token in the top right corner by clicking on Authorize button, then enter your token in the field provided in the format <code>Bearer &lt;token&gt;</code>.</p>"},{"location":"maintainer-guide/system-services/ai-ml-api/api-usage/endpoints/#versioning","title":"Versioning","text":"<p>We use semantic versioning for our API to ensure backward compatibility and clear communication of changes. Each endpoint's version and deprecation schedules are documented. </p>"},{"location":"maintainer-guide/system-services/ai-ml-api/api-usage/endpoints/#detailed-error-messages","title":"Detailed Error Messages","text":"<p>Our API provides detailed error messages to help developers troubleshoot issues quickly.</p>"},{"location":"maintainer-guide/system-services/ai-ml-api/api-usage/endpoints/#health-check-endpoint","title":"Health Check Endpoint","text":"<p>A health check endpoint is available to monitor the status of the API server. This endpoint can be used to verify that the server is running and responsive.</p>"},{"location":"maintainer-guide/system-services/ai-ml-api/api-usage/error-handling/","title":"Error Handling","text":"<ul> <li>HTTP Status Codes: <ul> <li>Using appropriate status codes to indicate the outcome of API requests.</li> </ul> </li> <li>Error Messages: <ul> <li>Providing clear and informative error messages for better troubleshooting.</li> </ul> </li> <li>Error Response Format: <ul> <li>Consistent format for error responses to facilitate client-side handling.</li> </ul> </li> <li>Best Practices: <ul> <li>Guidelines for handling and troubleshooting errors effectively.</li> </ul> </li> <li>Logging: <ul> <li>Logging errors for monitoring and debugging purposes.</li> </ul> </li> <li>User-Friendly Messages: <ul> <li>Ensuring error messages are user-friendly and informative.</li> </ul> </li> <li>Documentation: <ul> <li>Documenting common errors and their resolutions for reference.</li> </ul> </li> <li>Testing: <ul> <li>Testing error scenarios to validate error handling mechanisms.</li> </ul> </li> </ul>"},{"location":"maintainer-guide/system-services/ai-ml-api/security/authentication-and-authorization/","title":"Authentication and Authorization","text":""},{"location":"maintainer-guide/system-services/ai-ml-api/security/authentication-and-authorization/#overview","title":"Overview","text":"<p>This document outlines the authentication and authorization mechanisms implemented in our system. It covers the token-based security mechanism using an Envoy filter, integration with Keycloak for identity and access management, and the application of Role-based Access Control (RBAC) for fine-grained permissions.</p>"},{"location":"maintainer-guide/system-services/ai-ml-api/security/authentication-and-authorization/#token-based-security-mechanism","title":"Token-based Security Mechanism","text":"<p>The system employs a robust token-based security mechanism integrated within the Istio service mesh. This approach ensures that only authenticated and authorized requests can access protected services.</p>"},{"location":"maintainer-guide/system-services/ai-ml-api/security/authentication-and-authorization/#implementation-details","title":"Implementation Details","text":"<ul> <li>Envoy Filter: A custom HTTP filter deployed as a sidecar container alongside the service it secures. It performs the following functions:</li> <li>Request Interception: All incoming requests are intercepted for validation.</li> <li>Token Validation: Validates the JWT token's signature and checks the token ID against a revocation list stored in a PostgreSQL database.</li> <li>Caching: To improve performance and reduce database queries, caching is used for the token's revocation status and the public key for signature verification.</li> </ul>"},{"location":"maintainer-guide/system-services/ai-ml-api/security/authentication-and-authorization/#verification-process","title":"Verification Process","text":"<ol> <li>Extract Token: The token is extracted from the request header.</li> <li>Validate Structure and Signature: Checks if the token structure is valid and verifies the signature.</li> <li>Check Expiration: Ensures the token has not expired.</li> <li>Verify Claims: Verifies the issuer and audience claims.</li> <li>Revocation List Check: Checks the token ID against a revocation list.</li> </ol>"},{"location":"maintainer-guide/system-services/ai-ml-api/security/authentication-and-authorization/#keycloak-integration","title":"Keycloak Integration","text":"<p>Keycloak serves as the central identity and access management system, offering features like OAuth 2.0, OpenID Connect support, and multi-factor authentication.</p>"},{"location":"maintainer-guide/system-services/ai-ml-api/security/authentication-and-authorization/#integration-steps","title":"Integration Steps","text":"<ol> <li>Realm Configuration: Set up a Keycloak realm specific to the API.</li> <li>Client Application Setup: Register the client application in Keycloak.</li> <li>Roles and Permissions: Define roles and permissions for access control.</li> <li>Token Configuration: Configure token settings, including lifespan and signature algorithm.</li> <li>Library Integration: Integrate Keycloak libraries with the Envoy filter for seamless authentication and authorization.</li> </ol>"},{"location":"maintainer-guide/system-services/ai-ml-api/security/authentication-and-authorization/#access-control","title":"Access Control","text":"<p>Role-based Access Control (RBAC) is implemented to provide fine-grained access control to API endpoints. This section could be expanded with examples of role definitions and how they are applied to endpoints.</p>"},{"location":"maintainer-guide/system-services/ai-ml-api/security/authentication-and-authorization/#best-practices","title":"Best Practices","text":"<ul> <li>Regularly Update Roles: Ensure roles and permissions are regularly reviewed and updated to reflect changes in access requirements.</li> </ul>"},{"location":"maintainer-guide/system-services/ai-ml-api/security/authentication-and-authorization/#common-issues-and-troubleshooting","title":"Common Issues and Troubleshooting","text":"<ul> <li>Token Validation Failure: Ensure the token has not expired and the signature matches the public key.</li> <li>Access Denied: Verify the user's roles and permissions align with the requested resource's access control policies.</li> <li>Performance Issues: Check the caching mechanism for token validation and revocation status to ensure it is functioning correctly.</li> </ul>"},{"location":"maintainer-guide/system-services/ai-ml-api/security/encryption/","title":"Encryption","text":"<p>The implementation of the Alternative Envoy filter within an Istio service mesh is a cornerstone for ensuring robust encryption practices, thereby guaranteeing data security across the network. This document outlines the encryption strategies employed to protect data both in transit and at rest, emphasizing the importance of these measures in maintaining data integrity and confidentiality within a distributed system.</p>"},{"location":"maintainer-guide/system-services/ai-ml-api/security/encryption/#secure-data-transmission-with-https","title":"Secure Data Transmission with HTTPS","text":"<p>HTTPS is utilized as the primary means for secure data transmission. By leveraging Transport Layer Security (TLS), HTTPS provides a secure channel over which data can be transmitted between clients and services. This prevents data interception and tampering by encrypting the data during transit.</p>"},{"location":"maintainer-guide/system-services/ai-ml-api/security/encryption/#benefits-of-https","title":"Benefits of HTTPS:","text":"<ul> <li>Encryption: Ensures that data exchanged between the client and server is encrypted, protecting against eavesdropping and man-in-the-middle attacks.</li> <li>Data Integrity: Guarantees that the data sent is not altered or corrupted during transfer.</li> <li>Authentication: Verifies that the users are communicating with the intended website, building trust.</li> </ul>"},{"location":"maintainer-guide/system-services/ai-ml-api/security/encryption/#encryption-of-sensitive-data","title":"Encryption of Sensitive Data","text":"<p>Sensitive information, including JWT tokens and revocation lists, is encrypted not only in transit but also at rest. This dual-layer encryption strategy provides comprehensive protection against unauthorized access and potential data breaches.</p>"},{"location":"maintainer-guide/system-services/ai-ml-api/security/encryption/#data-in-transit","title":"Data in Transit:","text":"<ul> <li>End-to-End Encryption: For sensitive payloads, end-to-end encryption is employed, ensuring that data is encrypted from the source all the way to the destination without being decrypted at intermediary points.</li> </ul>"},{"location":"maintainer-guide/system-services/ai-ml-api/security/encryption/#encryption-algorithms-and-key-management","title":"Encryption Algorithms and Key Management","text":"<p>Choosing the right encryption algorithms and managing encryption keys effectively are critical aspects of a robust encryption strategy.</p> <ul> <li>Algorithms: AES (Advanced Encryption Standard) for data at rest and TLS 1.3 for data in transit are recommended due to their strong security features and widespread support.</li> <li>Key Management: Securely managing the keys involves generating, storing, rotating, and retiring encryption keys in a secure manner. A centralized key management system can help automate these processes, reducing the risk of human error.</li> </ul>"},{"location":"maintainer-guide/system-services/ai-ml-api/token-management/ckan-integration/","title":"Token Management with CKAN","text":""},{"location":"maintainer-guide/system-services/ai-ml-api/token-management/ckan-integration/#integration-with-ckan","title":"Integration with CKAN","text":"<p>The integration is achieved through a combination of custom development and configuration, ensuring a seamless experience for both administrators and end-users.</p>"},{"location":"maintainer-guide/system-services/ai-ml-api/token-management/ckan-integration/#custom-ckan-extension","title":"Custom CKAN Extension","text":"<p>A custom CKAN extension has been developed to bridge CKAN with Keycloak, enabling advanced token management features. This extension allows for:</p> <ul> <li>Seamless synchronization between CKAN user accounts and Keycloak authentication.</li> <li>Enhanced security measures through Keycloak's robust authentication mechanisms.</li> </ul>"},{"location":"maintainer-guide/system-services/ai-ml-api/token-management/ckan-integration/#database-schema","title":"Database Schema","text":"<p>The CKAN database schema has been extended to support the storage of token metadata, including:</p> <ul> <li>Token identifiers and associated user accounts.</li> <li>Token scopes, expiration dates, and creation timestamps.</li> </ul>"},{"location":"maintainer-guide/system-services/ai-ml-api/token-management/ckan-integration/#user-interface-enhancements","title":"User Interface Enhancements","text":"<p>To improve the user experience, we have introduced new UI components focused on token management within the CKAN platform.</p>"},{"location":"maintainer-guide/system-services/ai-ml-api/token-management/ckan-integration/#new-ui-components","title":"New UI Components","text":"<ul> <li>Token Management Dashboard: Integrated into the user profile, this dashboard provides a comprehensive overview of a user's tokens, including creation dates, scopes, and expiration dates.</li> <li>Token Creation Wizard: A user-friendly interface that guides users through the process of creating new tokens, with options to customize scopes and set expiration dates.</li> <li>Token Revocation and Renewal Interface: Easy-to-use interface that allow users to revoke or renew their tokens.</li> </ul>"},{"location":"maintainer-guide/system-services/ai-ml-api/token-management/ckan-integration/#accessibility","title":"Accessibility","text":"<p>In line with our commitment to inclusivity, the new UI components are designed to be accessible to all users, ensuring a seamless experience regardless of individual needs or preferences.</p>"},{"location":"maintainer-guide/system-services/ai-ml-api/token-management/token-lifecycle/","title":"Token Lifecycle Management","text":""},{"location":"maintainer-guide/system-services/ai-ml-api/token-management/token-lifecycle/#obtaining-tokens","title":"Obtaining Tokens","text":"<ol> <li>User initiates token creation through CKAN interface</li> <li>CKAN validates user permissions</li> <li>Token request sent to Keycloak with specified parameters</li> <li>Keycloak generates JWT with appropriate claims</li> <li>Token metadata stored in CKAN database</li> <li>Token presented to user (displayed only once for security)</li> </ol>"},{"location":"maintainer-guide/system-services/ai-ml-api/token-management/token-lifecycle/#revoking-tokens","title":"Revoking Tokens","text":"<ol> <li>The user revokes an existing access token through the CKAN platform.</li> <li>CKAN adds the revoked token to a list of invalid tokens stored in the PostgreSQL database.</li> <li>CKAN sends a request to the Envoy Filter to refresh the cache of revoked tokens.</li> </ol>"},{"location":"maintainer-guide/system-services/ai-ml-api/token-management/token-lifecycle/#expiration-policies","title":"Expiration Policies","text":"<ul> <li>Default token lifespan: 6 months (configurable)</li> </ul>"},{"location":"user-guide/getting-started/","title":"Getting started","text":"<p>This user guide provides instructions for using the Data Management Platform\u2019s web interface to organize, publish, and discover data. In addition to the web interface, the platform offers a robust API that facilitates the development of extensions and integration with other information systems.</p> <p>The platform also integrates JupyterHub, which enables users to interact with datasets and perform computations within isolated virtual environments. JupyterHub provides a familiar notebook-based interface suitable for data analysis, preprocessing, and exploratory tasks.</p> <p>This guide also provides instructions for end users on how to work with the integrated AI/ML models available on the platform. It covers how to access the models, submit data for processing, and interpret the results, all through a simplified and intuitive interface\u2014no programming or technical expertise is required.</p>"},{"location":"user-guide/model-usage/","title":"Overview","text":"<p>The ALTERNATIVE platform is designed to provide remote access to AI/ML models through a well-documented API and model runtime environment, enabling seamless interaction for end users. This approach allows researchers, regulators, and other stakeholders to execute predictive models, submit input data, and retrieve computational results programmatically without requiring local installations. The platform is optimized for scalability and security, ensuring efficient execution of different types of models such as physiologically based kinetic (PBK) models, toxicodynamic (TD) models, and QSAR-based AI models. However, the platform does not include UI-based client applications for visual analysis, as it prioritizes an API-driven workflow for integration with existing analytical pipelines and external applications. Users are encouraged to integrate API outputs with custom visualization tools, data processing environments, or third-party platforms suited to their specific needs.</p> <p>This section outlines the step-by-step procedure how to obtaining an authentication token, and interacting with the ALTERNATIVE platform\u2019s AI/ML models via the API.</p>"},{"location":"user-guide/model-usage/#usage-procedure","title":"Usage procedure","text":""},{"location":"user-guide/model-usage/#user-provisioning","title":"User provisioning","text":"<p>User accounts are provisioned by an administrator in Keycloak, where each user is assigned with email address, username, first and last name, and an initial password. Upon first login, users must verify their email address and are required to change their initial password to enhance security. The admin assigns specific roles based on the user type, such as access to AI/ML APIs or Jupyter Notebook environments. These roles define the level of access and permissions within the ALTERNATIVE platform.</p>"},{"location":"user-guide/model-usage/#login-in-web-platform","title":"Login in web platform","text":"<p>Once provisioned, users can log in to the ALTERNATIVE web platform using their username and password. The authentication system ensures secure access, and users are redirected to their dashboard upon successful login. After their first login, users must complete email verification and password change before gaining full access to assigned services.</p>"},{"location":"user-guide/model-usage/#generate-aiml-token","title":"Generate AI/ML Token","text":"<p>To interact with the ALTERNATIVE APIs, users need an API token for authentication. This token is generated through the platform\u2019s user interface. Users must navigate to the token management section, where they can generate, renew, or revoke access tokens. The generated token is required to authenticate API requests securely. You can find instruction how to obtain AI/ML API Tokens Here</p>"},{"location":"user-guide/model-usage/#navigate-to-swagger-or-use-curl","title":"Navigate to Swagger or use CURL","text":"<p>The ALTERNATIVE platform provides an API documentation interface (Swagger UI) where users can explore available endpoints, review input/output structures, and test requests interactively. Alternatively, users can execute API calls programmatically using CURL or other HTTP clients such as Postman. Swagger UI allows users to interact with the API through a graphical interface, while CURL provides a command-line-based approach. Access the interactive documentation Here.</p>"},{"location":"user-guide/model-usage/#authenticate-with-the-token-in-client","title":"Authenticate with the token in client","text":"<p>To access the API, users must include the generated authentication token in their requests. This is typically done by adding the token to the Authorization header in API requests. When using Swagger, users can enter the token in the provided authentication field. In CURL, the token is passed using the -H 'Authorization: Bearer ' header."},{"location":"user-guide/model-usage/#enter-input-parameters","title":"Enter input parameters","text":"<p>Each AI/ML model within the ALTERNATIVE platform requires specific input parameters for execution. These parameters can include chemical structures (SMILES), molecular descriptors, physiological properties, or other relevant data. Users should refer to the API documentation (Swagger UI) to understand the required inputs for each endpoint.</p>"},{"location":"user-guide/model-usage/#execute-request-against-the-api","title":"Execute request against the API","text":"<p>After entering the required input parameters, users can submit a request to the API. The API processes the request, executes the corresponding AI/ML model, and returns prediction results. The response includes toxicity classifications, probability scores, simulated concentration-time profiles, or other model outputs. Users can retrieve and analyze the results either via Swagger UI, CURL, or programmatically through an API client.</p>"},{"location":"user-guide/ckan/api/","title":"Platform API","text":""},{"location":"user-guide/ckan/api/#overview","title":"Overview","text":"<p>The Platform API is a powerful interface that allows programmatic access to its's functionality. It enables developers and data scientists to interact with datasets, resources, and other platform features without using the web interface. The API is RESTful, using HTTP methods to perform operations and returning results in JSON format.</p> <p>ALTERNATIVE platform exposes most of its features through the API, making it the preferred method for working with datasets, especially for automation and integration purposes. </p>"},{"location":"user-guide/ckan/api/#key-api-functionalities","title":"Key API Functionalities","text":"<p>Platform's API offers a wide range of functionalities, including:</p> <ol> <li>Dataset Management: Create, read, update, and delete datasets.</li> <li>Resource Management: Add, modify, and remove resources within datasets.</li> <li>User and Organization Management: Manage user accounts and organizations.</li> <li>Search and Discovery: Search for datasets and resources using various parameters.</li> <li>Metadata Manipulation: Update and retrieve metadata for datasets and resources.</li> </ol>"},{"location":"user-guide/ckan/api/#common-api-endpoints-and-their-uses","title":"Common API Endpoints and Their Uses","text":"<p>Here are some of the most commonly used API endpoints and their functions:</p>"},{"location":"user-guide/ckan/api/#1-package-dataset-api","title":"1. Package (Dataset) API","text":"<ul> <li><code>package_list</code>: List all datasets in the ALTERNATIVE platform.</li> <li><code>package_show</code>: Retrieve the metadata of a dataset.</li> <li><code>package_create</code>: Create a new dataset.</li> <li><code>package_update</code>: Update an existing dataset.</li> <li><code>package_delete</code>: Delete a dataset.</li> </ul>"},{"location":"user-guide/ckan/api/#2-resource-api","title":"2. Resource API","text":"<ul> <li><code>resource_show</code>: Get metadata about a resource.</li> <li><code>resource_create</code>: Add a new resource to a dataset.</li> <li><code>resource_update</code>: Update the metadata of a resource.</li> <li><code>resource_delete</code>: Delete a resource from a dataset.</li> </ul>"},{"location":"user-guide/ckan/api/#3-user-api","title":"3. User API","text":"<ul> <li><code>user_list</code>: List users on the Platform site.</li> <li><code>user_show</code>: Return a user account.</li> <li><code>user_create</code>: Create a new user.</li> </ul>"},{"location":"user-guide/ckan/api/#4-organization-api","title":"4. Organization API","text":"<ul> <li><code>organization_list</code>: List or search organizations.</li> <li><code>organization_show</code>: Return the details of an organization.</li> <li><code>organization_create</code>: Create a new organization.</li> </ul>"},{"location":"user-guide/ckan/api/#api-usage-examples","title":"API Usage Examples","text":""},{"location":"user-guide/ckan/api/#python-examples","title":"Python Examples","text":"<p>Here are some examples of how to use the API with Python:</p> <ul> <li> <p>Listing all datasets:</p> <pre><code>import requests\n\nurl = 'https://platform.alternative-project.eu/api/3/action/package_list'\nresponse = requests.get(url)\ndatasets = response.json()['result']\nprint(datasets)\n</code></pre> </li> <li> <p>Retrieving dataset metadata:</p> <pre><code>import requests\n\nurl = 'https://platform.alternative-project.eu/api/3/action/package_show'\nparams = {'id': 'dataset-name-or-id'}\nresponse = requests.get(url, params=params)\ndataset_metadata = response.json()['result']\nprint(dataset_metadata)\n</code></pre> </li> <li> <p>Creating a new dataset:</p> <pre><code>import requests\n\nurl = 'https://platform.alternative-project.eu/api/3/action/package_create'\ndata = {\n    'name': 'my-new-dataset',\n    'title': 'My New Dataset',\n    'owner_org': 'my-organization'\n}\nheaders = {'Authorization': 'your-api-key'}\nresponse = requests.post(url, json=data, headers=headers)\nprint(response.json())\n</code></pre> </li> </ul>"},{"location":"user-guide/ckan/api/#curl-examples","title":"Curl Examples","text":"<p>Here are examples of how to use the API with curl:</p> <ul> <li> <p>Listing all datasets:</p> <pre><code>curl -X GET https://platform.alternative-project.eu/api/3/action/package_list\n</code></pre> </li> <li> <p>Retrieving dataset metadata:</p> <pre><code>curl -X GET https://platform.alternative-project.eu/api/3/action/package_show -d '{\"id\":\"dataset-name-or-id\"}'\n</code></pre> </li> <li> <p>Creating a new dataset:</p> <pre><code>curl -X POST https://platform.alternative-project.eu/api/3/action/package_create \\\n    -H \"Authorization: your-api-key\" \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"name\": \"my-new-dataset\", \"title\": \"My New Dataset\", \"owner_org\": \"my-organization\"}'\n</code></pre> </li> <li> <p>Updating a dataset:</p> <pre><code>curl -X POST https://platform.alternative-project.eu/api/3/action/package_update \\\n    -H \"Authorization: your-api-key\" \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"id\": \"existing-dataset-id\", \"title\": \"Updated Dataset Title\"}'\n</code></pre> </li> <li> <p>Deleting a dataset:</p> <pre><code>curl -X POST https://platform.alternative-project.eu/api/3/action/package_delete \\\n    -H \"Authorization: your-api-key\" \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"id\": \"dataset-to-delete-id\"}'\n</code></pre> </li> </ul> <p>For more detailed information about the API, including all available endpoints and their parameters, please refer to the official CKAN API documentation.</p>"},{"location":"user-guide/ckan/datasets/","title":"Datasets, Resources, and Groups","text":""},{"location":"user-guide/ckan/datasets/#general-information","title":"General Information","text":"<p>The data management system underlying the ALTERNATIVE platform supports robust data sharing capabilities. It offers a web-based interface for cataloging, storing, and accessing data, enabling organizations to efficiently publish and share information with internal stakeholders or external audiences.</p>"},{"location":"user-guide/ckan/datasets/#datasets","title":"Datasets","text":"<p>Datasets are the core units of data in the platform. A dataset is a collection of data resources (such as files, databases, or APIs) along with descriptive metadata:</p> <ul> <li>Represent a logical collection of related data resources</li> <li>Contain metadata describing the data, its source, and usage</li> <li>Can be public or private, depending on sharing settings</li> <li>Are typically owned by an organization or user</li> <li>Can be tagged, categorized, and grouped for easy discovery</li> </ul>"},{"location":"user-guide/ckan/datasets/#resources","title":"Resources","text":"<p>Resources are the actual data files or references that make up a dataset.</p> <ul> <li>A resource can be any type of file (CSV, Excel, PDF, etc.) or a link to an external data source</li> <li>Multiple resources can be added to a single dataset</li> <li>Each resource has its own metadata, separate from the dataset metadata</li> <li>Resources can be previewed, downloaded, or accessed via API depending on their type and configuration</li> </ul>"},{"location":"user-guide/ckan/datasets/#groups","title":"Groups","text":"<p>Groups are collections of datasets that share a common theme. They provide an additional way to organize and categorize datasets beyond organizations. Key features of groups include:</p> <ul> <li>Datasets from different organizations can be part of the same group</li> <li>A dataset can belong to multiple groups</li> <li>Groups can have their own pages with description and branding</li> <li>They help in discovering related datasets across organizational boundaries</li> <li>Groups can have members with different levels of access</li> </ul>"},{"location":"user-guide/ckan/datasets/#creating-a-new-dataset","title":"Creating a New Dataset","text":"<p>To create a new dataset in the ALTERNATIVE platform:</p> <ol> <li>From the 'Datasets' page, click the 'Add Dataset' button, or go to your organization's page and click 'Add Dataset'.</li> <li>Fill in the required metadata fields:<ul> <li>Title (unique across the platform)</li> <li>Description</li> <li>Tags (for improved searchability)</li> <li>License</li> <li>Organization (dataset owner)</li> <li>Visibility (public or private)</li> </ul> </li> <li>Click 'Next: Add Data'.</li> <li>Add one or more resources to your dataset:<ul> <li>Upload a file or provide a link to the data</li> <li>Give the resource a name</li> <li>Provide a description of the resource</li> <li>Specify the format of the resource</li> </ul> </li> <li>Click 'Finish' when you've added all resources.</li> </ol>"},{"location":"user-guide/ckan/datasets/#managing-datasets-and-resources","title":"Managing Datasets and Resources","text":""},{"location":"user-guide/ckan/datasets/#editing-datasets","title":"Editing Datasets","text":"<p>To edit an existing dataset:</p> <ol> <li>Navigate to the dataset's page.</li> <li>Click 'Manage'.</li> <li>In the 'Edit metadata' tab, you can modify any of the fields.</li> <li>Click 'Update Dataset' to save your changes.</li> </ol>"},{"location":"user-guide/ckan/datasets/#managing-resources","title":"Managing Resources","text":"<p>To manage resources within a dataset:</p> <ol> <li>Go to the dataset's 'Manage' page.</li> <li>Click on the 'Resources' tab.</li> <li>Here you can:</li> <li>Add new resources by clicking 'Add new resource'</li> <li>Edit existing resources by selecting them and modifying the information</li> <li>Delete resources if needed</li> </ol>"},{"location":"user-guide/ckan/datasets/#dataset-collaborators","title":"Dataset Collaborators","text":"<p>You can manage collaborators for a dataset:</p> <ol> <li>In the dataset's 'Manage' page, go to the 'Collaborators' tab.</li> <li>Add new collaborators or edit/delete existing ones.</li> <li>Assign roles: Member (view only), Editor (can edit), or Admin (full control).</li> </ol>"},{"location":"user-guide/ckan/datasets/#groups-management","title":"Groups Management","text":""},{"location":"user-guide/ckan/datasets/#creating-a-group","title":"Creating a Group","text":"<p>Note: Only system administrators can create groups in the ALTERNATIVE platform.</p> <p>To create a new group:</p> <ol> <li>Go to the 'Groups' page and click 'Add Group'.</li> <li>Fill in the required information:</li> <li>Name (used in URLs, can only contain lowercase letters, numbers, and dashes)</li> <li>Description</li> <li>Optionally, add an image to represent the group.</li> <li>Click 'Create Group'.</li> </ol>"},{"location":"user-guide/ckan/datasets/#adding-datasets-to-a-group","title":"Adding Datasets to a Group","text":"<p>To add a dataset to a group:</p> <ol> <li>Go to the dataset's page.</li> <li>Click on the 'Groups' tab.</li> <li>Select the group you want to add the dataset to from the dropdown menu.</li> <li>Click 'Add to group'.</li> </ol>"},{"location":"user-guide/ckan/datasets/#managing-group-membership","title":"Managing Group Membership","text":"<p>Group administrators can manage group membership:</p> <ol> <li>Go to the group's page and click 'Manage'.</li> <li>In the 'Members' tab, you can:</li> <li>Add new members by entering their username and selecting a role</li> <li>Change roles of existing members</li> <li>Remove members from the group</li> </ol> <p>Group roles include: - Member: Can see the group's private datasets - Editor: Can add/remove datasets from the group - Admin: Can manage group membership and edit group properties</p>"},{"location":"user-guide/ckan/datasets/#metadata","title":"Metadata","text":"<p>Metadata is crucial for organizing and discovering datasets. The ALTERNATIVE platform uses three types of metadata:</p> <ol> <li>Default Metadata: Includes fields like title, description, tags, license, etc.</li> <li>Advanced Metadata for Experiments: Custom fields for scientific experiments (e.g., culture medium, number of replicates, toxin type).</li> <li>Custom Metadata: Arbitrary key/value pairs for additional dataset information.</li> </ol>"},{"location":"user-guide/ckan/datasets/#best-practices","title":"Best Practices","text":"<ul> <li>Use clear, descriptive titles for datasets and resources.</li> <li>Provide comprehensive descriptions to aid in discovery and understanding.</li> <li>Use relevant tags to improve searchability.</li> <li>Always include licensing information.</li> <li>Keep metadata up-to-date, especially for frequently updated datasets.</li> <li>Use appropriate file formats for resources to ensure accessibility.</li> <li>Use groups to create meaningful collections of datasets that span multiple organizations.</li> <li>Keep group descriptions up-to-date to help users understand the purpose and content of the group.</li> <li>Regularly review group memberships to ensure appropriate access levels.</li> </ul> <p>By following these guidelines, you can effectively create, manage, and share datasets, resources, and groups in the ALTERNATIVE platform, facilitating collaboration and data accessibility across the project.</p>"},{"location":"user-guide/ckan/metadata/","title":"Metadata","text":""},{"location":"user-guide/ckan/metadata/#overview","title":"Overview","text":"<p>Metadata is structured reference data that helps to sort and identify the information it describes. In the context of our platform, metadata plays a crucial role in organizing, finding, and understanding datasets. This guide covers the various metadata fields available, including standard fields, advanced fields for experimental data, and custom fields. It also explains how to effectively use metadata for searching and filtering datasets within the platform. Understanding and properly utilizing metadata is essential for researchers and data scientists to efficiently manage and access the wealth of information stored in the ALTERNATIVE platform.</p>"},{"location":"user-guide/ckan/metadata/#fields","title":"Fields","text":"<p>Each dataset has metadata associated with it. The following fields are available for describing datasets:</p> <ul> <li>Title - title will be unique across the platform, so make it brief but specific</li> <li>URL - automatically filled based on title, but can be edited</li> <li>Description - add a longer description of the dataset, including information such as what people need to know when using the data</li> <li>Tags - add tags that will help people find the data and link it with other related data</li> <li>License - it is important to include a license, so that people know how they can use the data</li> <li>Organization - choose which organization, that you're a member of, should own the dataset</li> <li>Visibility - a public dataset can be seen by anyone, a private one can only be seen by members of the organization owning the dataset or by collaborators of the dataset, and will not show up in searches by other users</li> <li>Source - where the data is from</li> <li>Version - version of the data</li> <li>Author - name of the person or organization responsible for producing the data</li> <li>Author e-mail - an e-mail address for the author, to which queries about the data should be sent</li> <li>Maintainer - name of the person or organization responsible for maintaining the data</li> <li>Maintainer e-mail - details for a second person responsible for the data</li> </ul>"},{"location":"user-guide/ckan/metadata/#advanced-metadata-for-experiments","title":"Advanced Metadata for Experiments","text":"<p>When creating/editing a dataset, you can mark the checkbox to be able to set extra fields, related to experiment data:</p> <ul> <li>Culture medium</li> <li>Number of replicates</li> <li>Number of cells/well</li> <li>Ratio hiPSC-CMs/HCAECs</li> <li>Date of experiment</li> <li>Toxin</li> <li>Age type</li> <li>Dimension</li> <li>Category</li> <li>Content</li> <li>Model</li> </ul>"},{"location":"user-guide/ckan/metadata/#custom-fields","title":"Custom Fields","text":"<p>If you want the dataset metadata to have more fields, you can add a name/key and value for it.</p>"},{"location":"user-guide/ckan/metadata/#find-data","title":"Find Data","text":"<p>You should be able to find a dataset by typing the title, or some relevant words from the description/metadata, into the search box on any page, containing datasets. On the left side of the <code>Datasets</code> page there are also some filters, such as <code>Organizations</code>, <code>Groups</code>, <code>Tags</code>, <code>Formats</code>, <code>Licenses</code>. Select any number of options to restrict the search. Under the search box there's also an <code>Order by</code> field, to sort datasets in any given way. If you want to look for data owned by a particular organization/group, you can search within that from its homepage.</p> <p>The platform utilizes Apache Solr as its search engine. It is important to note that not all available functionality is exposed through the simplified search interface, and certain behaviors may differ. The system supports two search modes, both accessible from the same search field. When the entered search terms do not contain a colon (:), a simple search is performed. If the expression includes a colon, the system interprets it as an advanced search.</p>"},{"location":"user-guide/ckan/metadata/#simple-search","title":"Simple Search","text":"<p>The platform delegates most search operations to Apache Solr and, by default, uses the DisMax Query Parser, which is designed for ease of use and to accept a wide range of input formats.</p> <p>The user\u2019s input in the search field defines the core query. Certain characters are interpreted as modifiers: + denotes required terms, while - denotes prohibited terms. Text enclosed in balanced quotation marks (e.g., \"example text\") is treated as a phrase. By default, all specified words or phrases are considered optional unless explicitly marked with + or -. Searches are performed on complete words, and wildcard characters are not supported in simple search mode. Solr performs preprocessing and stemming during search operations\u2014this involves reducing words to their base form, so that searching for testing or tested may return results containing test. Additionally, in cases where dataset names include hyphens (-), each segment separated by the hyphen is treated as an independent searchable term. </p> <p>Examples:</p> <ul> <li><code>census</code> -&gt; will search for all the datasets containing the word <code>census</code> in the query fields</li> <li><code>census +2019</code> -&gt; will search for all the datasets contaning the word <code>census</code> and filter only those matching <code>2019</code> too</li> <li><code>census -2019</code> -&gt; will search for all the datasets containing the word <code>census</code> and will exclude the results featuring <code>2019</code></li> <li><code>\"european census\"</code> -&gt; will search for all the datasets containing the phrase <code>european census</code></li> <li><code>Testing</code> -&gt; will search for all the datasets containing the word <code>Testing</code> and also <code>Test</code> as it is the stem of <code>Testing</code></li> </ul>"},{"location":"user-guide/ckan/metadata/#advanced-search","title":"Advanced Search","text":"<p>This will be considered a fielded search and the query syntax of Solr will be used to search. This will allow us to use wildcards (<code>*</code>), proximity matching (<code>~</code>, looks for terms that are within a specific distance from one another) and general features described in Solr docs. The basic syntax is <code>field:term</code>. Field names may differ from datasets' attributes, the mapping rules are defined in the schema.xml file. You can use <code>title</code> to search by the dataset name and <code>text</code> to look in a catch-all field. The platform supports fuzzy searches based on the Levenshtein Distance, or Edit Distance algorithm, to do a fuzzy search use the <code>~</code> symbol at the end of a single-word term.</p> <p>Examples:</p> <ul> <li><code>title:european</code> -&gt; will look for all the datasets containing in their title the word <code>european</code></li> <li><code>title:europ*</code> -&gt; will look for all the datasets containing in their title a word that starts with <code>europ</code>, like <code>europe</code> and <code>european</code></li> <li><code>title:europe || title:australia</code> -&gt; will look for datasets containing <code>europe</code> or <code>australia</code> in their title</li> <li><code>title: \"european census\" ~ 4</code> -&gt; will look for datasets, in which the title contains the words <code>european</code> and <code>census</code> within a distance of 4 words</li> <li><code>author:powell~</code> -&gt; words like <code>jowell</code> or <code>pomell</code> will also be found</li> </ul>"},{"location":"user-guide/ckan/overview/","title":"Data Management Platform","text":""},{"location":"user-guide/ckan/overview/#overview","title":"Overview","text":"<p>The ALTERNATIVE platform is designed to facilitate easy creation, sharing, and management of various types of data, catering to both internal teams and external audiences. It offers a user-friendly web interface that streamlines the process of data publication and discovery.</p>"},{"location":"user-guide/ckan/overview/#key-objectives","title":"Key Objectives","text":"<ul> <li>Efficient Data Sharing: Enable seamless data exchange between consortium partners throughout the project's duration.</li> <li>Flexible Access Control: Implement granular permissions for viewing, editing, and managing datasets.</li> <li>Metadata Management: Provide comprehensive metadata functionality, including custom fields for scientific experiments.</li> <li>Integration with Development Environment: Seamless connection with JupyterHub for data analysis and model development.</li> <li>Secure Authentication: Utilize Keycloak for robust identity and access management.</li> <li>Extensibility: Utilize the platform's modular extension mechanism to customize functionality in alignment with ALTERNATIVE's specific requirements.</li> </ul>"},{"location":"user-guide/ckan/overview/#features","title":"Features","text":"<ul> <li>Organization-based Data Management: Each consortium partner is represented as an organization with unique workflows and authorization mechanisms.</li> <li>Rich Dataset Functionality: Support for various resource types, including CSV, Excel, XML, PDF, and image files.</li> <li>Advanced Metadata: Custom metadata fields for scientific experiments, enhancing data discoverability and understanding.</li> <li>Secure Collaboration: Granular access controls and dataset collaborator roles (Member, Editor, Admin).</li> <li>API Access: Comprehensive API for programmatic interaction with datasets and resources.</li> <li>Cloud Storage Integration: Utilization of S3 storage for efficient management of large datasets.</li> <li>Single Sign-On (SSO): Seamless authentication between Data Management and JupyterHub environments.</li> </ul>"},{"location":"user-guide/ckan/users/","title":"Users, Organizations and Authorization","text":""},{"location":"user-guide/ckan/users/#overview","title":"Overview","text":"<p>The Alternative platform implements a comprehensive user management and authorization system. This system defines different user types, roles within organizations, and access controls for datasets and features. Understanding these concepts is crucial for effectively using and managing the platform, whether you're a regular user, an organization member, or an administrator.</p> <p>There are 2 types of users: regular and administrator. An account is not required to search for and find data, unless the information is private, but it is needed for all publishing functions and personalization features. The platform uses Keycloak to manage user identity and access. You can find the <code>Log in</code> button from the top right corner of any page.</p>"},{"location":"user-guide/ckan/users/#profile","title":"Profile","text":"<p>You can see your profile by selecting the button containing your picture and name at the top of any page. You can change the information about you, including what other users see about you by editing your profile. To do this, select the gearwheel symbol at the top of any page or go to your profile page and select <code>Manage</code>. Make the changes you want and then press <code>Update Profile</code>.</p>"},{"location":"user-guide/ckan/users/#api-tokens","title":"API Tokens","text":"<p>API tokens are used in API calls or with the python library to access private datasets, that your account has rights to.</p> <p>To generate an API token:</p> <ol> <li>Go to your profile page</li> <li>Select the <code>API Tokens</code> tab</li> <li>Set a name for your token and click <code>Create API Token</code></li> <li>A green box appears above, containing your API token, make sure to copy it - you won't be able to see it again</li> </ol> <p>Underneath the creation form you can see all your tokens. From the list you can also <code>Revoke</code> a token, that you don't need anymore.</p>"},{"location":"user-guide/ckan/users/#aiml-api-tokens","title":"AI/ML API Tokens","text":"<p>AI/ML API tokens are used to access the AI/ML API. To generate an AI/ML API token:</p> <ol> <li>Go to your profile page</li> <li>Select the <code>AI/ML API Tokens</code> tab</li> <li>Set a name for your token and click <code>Create AI/ML API Token</code></li> <li>Set expiration date and roles for the token</li> <li>Click <code>Create AI/ML API Token</code></li> <li>A green box appears above, containing your AI/ML API token, make sure to copy it - you won't be able to see it again</li> </ol>"},{"location":"user-guide/ckan/users/#news-feed","title":"News Feed","text":"<p>At the top of any page, select the dashboard symbol. This shows changes to datasets, organizations and groups that you follow. It is possible to follow individual users to be notified of changes that they make. To start/stop following a dataset, organization, group, or user, go to their dedicated page and select <code>Follow</code>/<code>Unfollow</code>. You can also select the <code>Activity Stream</code> tab to see all activities related to the item. You can enable email notifications for updates to items you follow by enabling <code>Subscribe to notification emails</code> from your profile settings.</p>"},{"location":"user-guide/ckan/users/#organizations","title":"Organizations","text":"<p>Organizations have members and own datasets. You need to be a member of an organization in order to create datasets. Each organization can have its own workflow and authorizations, allowing it to manage its own publishing process. It also has a homepage, where users can find some information about the organization and search within its datasets, this can be accessed by going to <code>Organizations</code> and selecting the specific organization you want to explore. Only administrator users can create organizations, by going to the <code>Organizations</code> page and pressing <code>Add Organization</code>. Initially, the organization has no datasets and only 1 member - the creator.</p>"},{"location":"user-guide/ckan/users/#management","title":"Management","text":"<p>Users with the Admin role can edit an organization's information and change the access privileges to the organization for other users. You can do so by going to the organization's page and selecting the <code>Manage</code> button. The <code>Edit</code> tab lets you change organization information or <code>Delete</code> it. From the <code>Members</code> tab you can see a list of all members, add or remove users from the organization, or change their role.</p>"},{"location":"user-guide/ckan/users/#user-roles","title":"User Roles","text":"<p>User roles within an organization define the level of access and permissions a user has for that organization's resources. There are three roles, each building upon the permissions of the previous:</p> <ol> <li> <p>Member:</p> <ul> <li>Can view the organization's private datasets</li> <li>Can access and download data from the organization's datasets</li> <li>Cannot make changes to datasets or organization settings</li> </ul> </li> <li> <p>Editor:</p> <ul> <li>Has all the permissions of a Member</li> <li>Can create new datasets owned by the organization</li> <li>Can edit and update existing datasets</li> <li>Can publish or unpublish datasets</li> <li>Cannot modify organization settings or manage other users</li> </ul> </li> <li> <p>Admin:</p> <ul> <li>Has all the permissions of an Editor</li> <li>Can add new members to the organization</li> <li>Can remove members from the organization</li> <li>Can change roles of organization members</li> <li>Can modify organization settings and information</li> <li>Has full control over all datasets owned by the organization</li> </ul> </li> </ol> <p>To change a user's role: 1. Go to the organization's page 2. Click on the <code>Manage</code> button 3. Select the <code>Members</code> tab 4. Find the user in the list 5. Use the dropdown menu next to their name to select the new role 6. Click <code>Update Member</code></p>"},{"location":"user-guide/jupyterhub/bioconductor/","title":"Bioconductor","text":""},{"location":"user-guide/jupyterhub/bioconductor/#bioconductor","title":"Bioconductor","text":"<p>Installing and Using Bioconductor. When installing packages with <code>BiocManager::install()</code>, add the path to your R package directory, like so (replace <code>your_R_package_dir</code> with the directory name): <pre><code>BiocManager::install(lib=\"/home/jovyan/your_R_package_dir\")\n</code></pre> or <pre><code>BiocManager::install(\"package_name\", lib=\"/home/jovyan/your_R_package_dir\")\n</code></pre></p>"},{"location":"user-guide/jupyterhub/environments/","title":"Environments","text":""},{"location":"user-guide/jupyterhub/environments/#environments","title":"Environments","text":"<p>Every JupyterHub server is being spawned with two environments from the start - default python environment and an additional conda environment. On <code>Launcher</code> tab under <code>Notebook</code> or <code>Console</code> you can choose which environment to use. You can also use the <code>Terminal</code>:</p>"},{"location":"user-guide/jupyterhub/environments/#python-virtual-environments","title":"Python Virtual Environments","text":"<p>Each environment has their own independent set of Python packages installed in their <code>site</code> directories. A virtual environment is created on top of an existing Python installation, known as the virtual environment\u2019s <code>base</code> Python, and may be isolated from the packages in the base environment, so only those explicitly installed in the virtual environment are available. Python Virtual Environment Docs.</p> <p>Make sure you create these environments in the <code>/home/jovyan</code> directory or they will get deleted.</p> <ul> <li> <p>Add New Environments: <pre><code>python -m venv /path/to/new/virtual/environment\n</code></pre></p> </li> <li> <p>Activate Different Environment: <pre><code>source environment/bin/activate\n</code></pre></p> </li> <li> <p>Install Packages: <pre><code>pip install packageName\n</code></pre></p> </li> <li> <p>Uninstall Packages: <pre><code>pip uninstall packageName\n</code></pre></p> </li> <li> <p>Show Packages In Environment: <pre><code>pip list\n</code></pre></p> </li> <li> <p>Deactivate Current Environment: <pre><code>deactivate\n</code></pre></p> </li> <li> <p>Delete Environment: <pre><code>rm -rf environment\n</code></pre></p> </li> </ul>"},{"location":"user-guide/jupyterhub/environments/#conda-environments","title":"Conda Environments","text":"<p>Each environment has their own independent set of Python or Conda packages installed. Conda Environments Docs. Guide - using <code>pip</code> inside conda environment.</p> <p>Make sure you create these environments in the <code>/home/jovyan</code> directory or they will get deleted.</p> <p>You can check environment names and locations with: <pre><code>bash\nconda env list\n</code></pre></p> <ul> <li> <p>Add New Environments: <pre><code>conda create --prefix /home/jovyan/Conda2\n</code></pre></p> </li> <li> <p>Activate Different Environment: <pre><code>conda activate /home/jovyan/Conda2\n</code></pre></p> </li> <li> <p>Install Packages: <pre><code>conda install pip\npip install packageName\n</code></pre></p> </li> <li> <p>Uninstall Packages: <pre><code>pip uninstall packageName\n</code></pre></p> </li> <li> <p>Show Packages In Environment: <pre><code>conda list\npip list\n</code></pre></p> </li> <li> <p>Create Kernel From Environment: <pre><code>pip install ipykernel\npython -m ipykernel install --user --name=Conda2\n</code></pre></p> </li> <li> <p>Deactivate Current Environment: <pre><code>conda deactivate\n</code></pre></p> </li> <li> <p>Delete Environment: <pre><code>conda activate base\nconda remove -p /home/jovyan/Conda2 --all\njupyter kernelspec remove conda2\n</code></pre></p> </li> </ul>"},{"location":"user-guide/jupyterhub/filesystem/","title":"File System","text":""},{"location":"user-guide/jupyterhub/filesystem/#file-system","title":"File System","text":"<p>The <code>/home/jovyan</code> and <code>/home/shared</code> directories are not being deleted between server stops. <code>/home/jovyan</code> is personal for the user - nobody else can see its content; and has 10 GB available space. <code>/home/shared</code> is for all users - everybody can see its content; and has 20 GB available space. All other directories are deleted and recreated everytime the server is stopped/started. The folder <code>/home/jovyan/shared</code> is a symlink to <code>/home/shared</code> and can be used to interact with the files of the shared directory.</p>"},{"location":"user-guide/jupyterhub/library/","title":"Alternative Library","text":""},{"location":"user-guide/jupyterhub/library/#python-library","title":"Python Library","text":"<p>The python library alternative-lib is designed to help with finding datasets and downloading resources from them, by using ckanapi.</p> <p>It can be installed with: <pre><code>pip install alternative-lib\n</code></pre></p> <p>Example of the library being used to get a public dataset and download its resource.</p>"},{"location":"user-guide/jupyterhub/overview/","title":"JupyterHub","text":"<p>The platform integrates JupyterHub as a way to interact with datasets and virtual environments. To access JupyterHub select the <code>Jupyterhub</code> button from the top right corner of any page. Everytime a user visits the JupyterHub page, a JupyterLab server is spawned for them, which enables you to work with documents and activities such as Jupyter notebooks, text editors, files and terminals. Jupyter Notebooks (.ipynb files) are documents that combine runnable code with narrative text (Markdown), equations (LaTeX), images, interactive visualizations and more. Once your server is ready, you will be redirected to it. After some time of inactivity it will be shutdown.</p>"},{"location":"user-guide/jupyterhub/overview/#jupyterhub_1","title":"JupyterHub","text":"<p>At the top you can find different options and settings. You can get back to JupyterHub by selecting <code>File</code> -&gt; <code>Hub Control Panel</code> or press <code>Log Out</code> to end the session. At the bottom you can see how many terminals and consoles are open and what environment is being used. On the right there are debugging tools.</p> <p>On the left there's 4 tabs: - <code>File Browser</code> - interact with files or open a <code>Launcher</code> tab from the <code>+</code> button - <code>Running Terminals and Kernels</code> - see all open tabs and running kernels and terminals, option to close or stop any/all of them - <code>Table of Contents</code> - shows information about the currently open file/notebook - <code>Extension Manager</code> - can be used to install extensions</p> <p>In the middle, you can see the currently open tabs and interact with them. From the <code>Launcher</code> tab (create new one by clicking <code>+</code>), you can create notebooks or consoles, open terminals or start new text, markdown and python files. Example of a notebook file.</p>"},{"location":"user-guide/jupyterhub/r/","title":"R","text":""},{"location":"user-guide/jupyterhub/r/#r","title":"R","text":"<p>You can use R from a <code>Terminal</code>. The first time you install a new package you should be asked where to save it. Make sure any packages you install are in the <code>/home/jovyan</code> directory or they will get deleted.</p> <ul> <li> <p>Activate R Console: <pre><code>R\n</code></pre></p> </li> <li> <p>Help Command: <pre><code>help()\n</code></pre></p> </li> <li> <p>Install Package: <pre><code>install.packages(\"package_name\")\n</code></pre></p> </li> <li> <p>List Installed Packages: <pre><code>installed.packages()\n</code></pre></p> </li> <li> <p>Exit R Console: <pre><code>q()\n</code></pre></p> </li> </ul>"},{"location":"user-guide/jupyterhub/r/#useful-r-docs","title":"Useful R Docs","text":"<ul> <li>The R Project</li> <li>Kickstarting R</li> <li>Installing R Packages</li> </ul>"},{"location":"user-guide/jupyterhub/r/#version-update","title":"Version Update","text":"<p>The R version has been updated from 4.0.4 to 4.3.0. If you have installed any R packages before this update, you will need to add your previous package directory to R's paths, to be able to use those packages.</p> <p>Commands (should be executed in R console; replace <code>yourLib</code> with the path to your R package directory; by default that should have been <code>~/R/x86_64-pc-linux-gnu-library/4.0</code>):</p> <p>To add the directory in paths list: <pre><code>.libPaths( c( .libPaths(), \"yourLib\") )\n</code></pre></p> <p>To make the directory the main library for packages: <pre><code>.libPaths( c( \"yourLib\" , .libPaths() ) )\n</code></pre></p>"}]}